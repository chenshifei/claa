% File tacl2018v2.tex
% Sep 20, 2018

% The English content of this file was modified from various *ACL instructions
% by Lillian Lee and Kristina Toutanova
%
% LaTeXery is mostly all adapted from acl2018.sty.

\documentclass[11pt,a4paper]{article}
\usepackage{times,latexsym}
\usepackage{url}
\usepackage{CJKutf8}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{adjustbox}
\usepackage[T1]{fontenc}

%% Package options:
%% Short version: "hyperref" and "submission" are the defaults.
%% More verbose version:
%% Most compact command to produce a submission version with hyperref enabled
%%    \usepackage[]{tacl2018v2}
%% Most compact command to produce a "camera-ready" version
%%    \usepackage[acceptedWithA]{tacl2018v2}
%% Most compact command to produce a double-spaced copy-editor's version
%%    \usepackage[acceptedWithA,copyedit]{tacl2018v2}
%
%% If you need to disable hyperref in any of the above settings (see Section
%% "LaTeX files") in the TACL instructions), add ",nohyperref" in the square
%% brackets. (The comma is a delimiter in case there are multiple options specified.)

\usepackage[acceptedWithA]{tacl2018v2}




%%%% Material in this block is specific to generating TACL instructions
\usepackage{xspace,mfirstuc,tabulary}
\newcommand{\dateOfLastUpdate}{Sept. 20, 2018}
\newcommand{\styleFileVersion}{tacl2018v2}

\newcommand{\ex}[1]{{\sf #1}}

\newif\iftaclinstructions
\taclinstructionsfalse % AUTHORS: do NOT set this to true
\iftaclinstructions
\renewcommand{\confidential}{}
\renewcommand{\anonsubtext}{(No author info supplied here, for consistency with
TACL-submission anonymization requirements)}
\newcommand{\instr}
\fi

%
\iftaclpubformat % this "if" is set by the choice of options
\newcommand{\taclpaper}{final version\xspace}
\newcommand{\taclpapers}{final versions\xspace}
\newcommand{\Taclpaper}{Final version\xspace}
\newcommand{\Taclpapers}{Final versions\xspace}
\newcommand{\TaclPapers}{Final Versions\xspace}
\else
\newcommand{\taclpaper}{submission\xspace}
\newcommand{\taclpapers}{{\taclpaper}s\xspace}
\newcommand{\Taclpaper}{Submission\xspace}
\newcommand{\Taclpapers}{{\Taclpaper}s\xspace}
\newcommand{\TaclPapers}{Submissions\xspace}
\fi

%%%% End TACL-instructions-specific macro block
%%%%

\title{Bilingual Tweets Authorship Attribution}

% Author information does not appear in the pdf unless the "acceptedWithA" option is given
% See tacl2018v2.sty for other ways to format author information
\author{
 Shifei Chen \\
 Department of Linguistics and Philology/Uppsala University, Sweden \\
  {\sf Shifei.Chen.2701@student.uu.se} \\
}

\date{}

\begin{document}
\maketitle
\begin{abstract}
  This document attempted to tackle the authorship attribution (AA) problem across different languages. Focusing on Chinese and English, I have examined the possibilities of three attribution models --- a Vanilla Model which counts the frequency of mixed language char and word n-grams, a Machine Translation model and an Aligned Word Embedding Model. The final result showed that the Vanilla Model achieved the best accuracy on a dataset of 50 authors. The Aligned Word Embedding Model is the second best, which reveals its potential to solve the cross-language authorship attribution (CLAA) problem.
\end{abstract}

\section{Introduction}

In recent years, political propaganda has moved a significant amount of resources onto social media in additional to tranditional mass media. On one hand, it makes people more conscious about the ongoing topics in politics, but on the other hand, it also creates more unconfirmed rumors, manufactured facts, ``fake news'' and other forms of biased information. Since then, there has been many attempts of applying NLP techniques to counter-attack the negative effects caused by the manufactured information published on the social media. Most of them analyzed stylometric features on monolingual data and showed promising potential in this field \cite{rocha2016authorship}.

One of the examples of the aforementioned political propaganda on social media was in June 2019, when the proposed anti-extradition law in Hong Kong had attracted great controversy on the social media. The political propoganda was so severe that Twitter had to suspend more than 1000 twitter accounts violating their platform manipulation policies\footnote{\raggedright\url{https://blog.twitter.com/en_us/topics/company/2019/information_operations_directed_at_Hong_Kong.html}}. A brief study by \citet{wood_mcminn_feng_2019} had revealed that the languages used in these tweets contents spaned across several languages. As propoganda and manufactured information are now reaching out more people around the world by using multiple languages, there is the need to develop authorship attribution (AA) techniques which could be applied to social media texts written in more than one language. In this project, I would like to see the possibilities of cross-language authorship attribution (CLAA) --- selecting Chinese and English as my experiment languages, by applying both a Machine Translation and an Aligned Word Embedding Model. I have also compared these two more sophiscated models with a Vanilla Model to see the possible improvements.

The rest of the paper is structured as follows: Section \ref{sec:related-work} descirbes related works in general AA and CLAA problems. Section \ref{sec:data} and Section \ref{sec:method} explain how the dataset is built as well as the three models I have designed. Section \ref{sec:results} showed and evaluated their corrsponding results. Finally in Section \ref{sec:conclusion} I summarized the whole project and looked at possible future works for CLAA.

\section{Related Work}\label{sec:related-work}

CLAA has received less attention compared to AA on monolingual languages. \citet{bogdanova2014cross} explored the possbility of applying machine translation to connect two languages, in combination of tranditional stylometric features such as word-level and char-level n-grams. Their best result was an accuracy of 0.88 on a dataset of novels written by 6 authors. There are also researches attempted to tackle the problem without bridging different languages by their semantics at all, such as \citet{llorens2016deep} and \citet{sarwar2018scalable} who both analyzed low level language independent features.

However, the dataset of all of the previous work consist of Indo-European lanuages, while in my case, the relationship between Chinese and English is much further than language pairs like Spanish and English. There are researches done for distant langauge pairs in cross-language plagiarm detection \cite{barron2010plagiarism}, but to my best knowledge no similar work for CLAA has been released.

Besides datasets, most AA tasks employed machine learning techniques as the classification algorithm, while there are few examples successfully applied Neural Networks to the same problem \cite{shrestha2017convolutional}. This is reasonable as monolingual AA usually catches the quirkiness of spelling, spacing or word richness of a specific writer, however in CLAA there might be no directly comparable features between distant languages pairs. Take Chinese and English as an example, it is impossible to compare the pattern of misspelling and spacing between these two languages because Chinese has no spelling and word separators. It is necessary for us to go up to the semantic level and look for similarities between texts to find out the real author.

\section{Data}\label{sec:data}

Since there is currently no publicly available corpus collecting social media texts from authors who write in both Chinese and English, I have built my own using Twitter Public APIs.

\begin{CJK*}{UTF8}{gbsn}

\begin{table}[t]
  \begin{center}
  \begin{tabular}{|l|l|}
  \hline \bf Chinese & \bf English \\ \hline
  是 & am, is, are \\
  的 & of \\
  有 & have, has \\
  在 & at, in, on \\
  \hline
  \end{tabular}
  \end{center}
  \caption{\label{tab:query-func-words-table} Function words used in the query string}
\end{table}

My first step was to build a query string to collect as many tweets in either English or Chinese as possible. I selected several most frequent function words in Chinese and their English counter parts, which are listed in Table \ref{tab:query-func-words-table}. Suppose $C=\{c_1, c_2, c_3, \cdots, c_i\}$ is a group of Chinese function words and $E=\{e_1, e_2, e_3, \cdots, e_j\}$ is the corrsponding English function words, the query string $Q$ would be the union set over the Cartesian product $C\times E$.

\begin{equation}
  Q=\bigcup_{k=1}s_k, s_k \in C \times E
\end{equation}

So if we select the first row in Table \ref{tab:query-func-words-table}, then our Chinese cadidate function word candidate would be ``是'' while the English candidates are ``am'', ``is'' and ``are''. The final query string would be \begin{verbatim}
  "(是 am) OR (是 is) OR (是 are)"
\end{verbatim}
\end{CJK*}

The next step was to fine grain all of the possible twitter users from the previous step. Here I defined two bilingual ratio values --- let $t_1$ and $t_2$ denote two sets of the tweets in any of the two language written by a user, $T$ is the set of all of his/her tweets. Then we have $R_i$ as the ratio between these two tweets languages.

\begin{equation}
  R_i = \frac{\min(|t_1|, |t_2|)}{\max(|t_1|, |t_2|)}
\end{equation}

And $R_T$ as the ratio between the bilingual content and the total tweets.

\begin{equation}
  R_T = \frac{|t_1|+|t_2|}{\text{|T|}}
\end{equation}

For each user, I crawled his or her first 200 tweets from his or her timeline (exclude retweets). The threshold were set at $R_i \ge 0.5$ and $R_T \ge 0.8$. In this way I could filter out two kinds of users --- someone who occasionally tweets in another language other than his/her main language, and someone who tweets in all kinds of languages.

After searching and fine graining I have selected 52 valid bilingual twitter accounts. Two of them were non-private accounts so I have removed them from the list, which shortened the length of the final list of bilingual twitter users to 50. I crawled all of their tweets and applied cleaning to these data, including removing URLs, hashtags, mentions, reserved words, emojis and smileys. The CNN classifier cannot take tweets that are shorter then 5 words so I have also segmented both Chinese and English tweets and removed those that were shorter than 5 segments. In the end, I have obtained 69710 tweets that were detailed in Table \ref{tab:overlook-dataset-table} and \ref{tab:dist-dataset-table}.

\begin{table}[t]
  \begin{center}
  \begin{tabular}{|l|r|}
  \hline \bf \# of tweets in total & 69710 \\
  \hspace{0.5cm} ZH tweets & 27476 \\
  \hspace{0.5cm} EN tweets & 37604 \\
  \hspace{0.5cm} Other lang. tweets & 4630 \\
  \hline
  \end{tabular}
  \end{center}
  \caption{\label{tab:overlook-dataset-table} Overall statistics of the dataset}
\end{table}

\begin{table*}[t]
  \begin{center}
  \begin{tabular}{|l|r|r|r|r|}
  \hline & \bf \# of tweets & \bf Length of raw tweets & \bf Length of ZH tweets & \bf length of EN tweets \\ \hline
  mean & 1383 & 100 & 38 & 73 \\
  std & 1211 & 38 & 29 & 31  \\
  min & 102 & 5 & 5 & 6 \\
  25\% & 335 & 68 & 16 & 47 \\
  50\% & 856 & 109 & 27 & 74 \\
  75\% & 2307 & 140 & 51 & 100 \\
  max & 4195 & 159 & 140 & 146 \\
  \hline
  \end{tabular}
  \end{center}
  \caption{\label{tab:dist-dataset-table} Statistics of the tweets per user}
\end{table*}

\section{Methodology}\label{sec:method}

\subsection{Splitting the Dataset}

Here each model is evaluated by 10-fold cross validation, except for the Aligned Word Embedding Model. It was trained and validated in fixed training, validataion and test datasets, which were derived from the full dataset by the ratio of 0.6, 0.1 and 0.3.

\subsection{Vanilla Model}

As \citet{rocha2016authorship} had showned in their work, the best stylometric features for AA on social media text are word-level and char-level n-grams. Since tranditional monolingual AA solution without semantic level features have already worked well, an intuitive solution to CLAA problems is to treat the multilingual dataset as a monolingual dataset, counting word-level and char-level stylometric features regardless of their languages. Another reason for this motivation is the charatiristic of the bilingual tweets. In the dataset many authors were mixing both languages in a single tweet and the boundary of different language contents varied from author to author. Some people perfer to use carriage return, some perfer a single whitespace, or some people just don't bother. Char-level n-grams should be able to pick up different language boundaries as a feature for author identification.

I have designed the vanilla attribution model with word-level 1, 2, 3-grams and char-level 1, 2, 3-grams, which was then fed into a logistic regression classifier and a LIBLINEAR SVM classifier \cite{REF08a} implemented by Scikit-learn \cite{scikit-learn}. The word-level and char-level features were counted from all available tweets. After experiments I found that TD-IDF values can improve the accuracy compare to pure word or char n-gram counts, so I have transformed all of the occurances to TD-IDF values as well.

We have seen from Table \ref{tab:dist-dataset-table} that the number of tweets from each user is highly unbalanced. The most frequent user tweeted 40 times more than the most quiet user. So I have set both of the logistic regression and SVM classifier to balance out the dataset automatically by assigning more weight to minor classes (authors).

\subsection{Machine Translation Model}

I have also adapted machine translation followed by \citet{bogdanova2014cross} to tackle the CLAA problem. I have used the Translator Text API from Microsoft Azure Cognitive Services, specifying the source language and the target langauge manually. One thing to note is that even the state-of-art machine translation service is still far from perfect and underperforms on social media texts compared to formal writings. In other words, it will inevitably introduce ``distortion'' to the raw tweets and worsen the result in theory.

In this second model, I have extracted and divided the raw tweets into the English group and the Chinese group. For tweets that mix both languages I seperated them into two tweets and put them into the Chinese and English group. Then tweets in each language group will be machine translated into the other language before being fed into the logistic regression and SVM classifier described in the Vanilla Model. They were classified among other translated tweets within the same language group together.

\subsection{Aligned Word Embedding Model}

The last model I have applied is a Convolutional Neural Network classifier inspired by \citet{shrestha2017convolutional}. They have proposed an architecture using char n-grams models as the single embedding layer to deal with tweets classification. But since my task is to explore the classification problem between two languages, I have replaced the char n-grams model with an aligned word embedding model. \citet{zou2013bilingual} claimed that such kind of model could correctly capture the semantic proximity between words in different languages no matter they have directly translation or not. They have improved the BLEU score of a machine translation task by nearly half point. In my case, as social media text is hard to normalize and translate, word embeddings in a unified semantic space could possibily perform better than machine translation. I have chosen the aligned fastText word embeddings \cite{bojanowski2017enriching} \cite{joulin2018loss} as my embedding layer since they provide both pre-trained word embeddings in both languages. A side note is that because fastText only provides word embeddings for Tranditional Chinese, I have used OpenCC\footnote{\url{https://github.com/BYVoid/OpenCC}} to convert all the Simplified Chinese content to Tranditional Chinese.

In the aligned fastText word embeddings, each word is represented by a 300 dimension vector. I concatenated two embeddings to form a 600 dimension word embedding for every word in the bilingual vocabulary, padded them and sent them to the next layer. Each OOV words are marked as \verb|<UNK>| and are giving an embedding of zeros.

The CNN network also has four convolutional layers, each of them has 100 kernels sizing from 2, 3, 4 or 5. They are designed to catch the information hidden inside the word 2, 3, 4 and 5-grams. I have also used the Adam optimizer \cite{kingma2014adam} together with other hyperparameters listed in Table \ref{tab:cnn-hyperparams-table}.

\begin{table}[t]
  \begin{center}
  \begin{tabular}{|l|l|}
  \hline \bf Hyperparameters & \bf Value \\ \hline
  \# of embedding layers & 1 \\
  \hspace{0.5cm} dimension & 600 \\
  \# of convolutional layers & 4 \\
  \hspace{0.5cm} kernel size & [2, 3, 4, 5] \\
  \hspace{0.5cm} \# of kernels & 100/layer \\
  \hspace{0.5cm} pooling & max \\
  Dropout & 0.5 \\
  Learning rate & 0.001 \\
  Max epochs & 10 \\
  Batch size & 32 \\
  \hline
  \end{tabular}
  \end{center}
  \caption{\label{tab:cnn-hyperparams-table} Hyperparameter settings in the Aligned Word Embedding Model}
\end{table}

\section{Results}\label{sec:results}

\subsection{Best Features}

\begin{table*}[t]
  \begin{center}
  \begin{adjustbox}{width={\textwidth},totalheight={\textheight},keepaspectratio}%
  \begin{tabular}{|l|r|r|r|r|r|r|}
  \hline & \bf LR & \bf SVM & \bf LR+MT(EN) & \bf SVM+MT(EN) & \bf LR+MT(ZH) & \bf SVM+MT(ZH) \\ \hline
  Word 1-gram & 0.66 & 0.71 & 0.53 \textcolor{gray}{(-19.1\%)} & 0.55 \textcolor{gray}{(-22.5\%)} & 0.61 \textcolor{gray}{(-6.8\%)} & 0.64 \textcolor{gray}{(-10.1\%)} \\
  \hspace{0.5cm} 2-gram & 0.65 & 0.74 & 0.54 \textcolor{gray}{(-16.2\%)} & 0.60 \textcolor{gray}{(-19.1\%)} & 0.61 \textcolor{gray}{(-5.6\%)} & 0.68 \textcolor{gray}{(-8.6\%)} \\
  \hspace{0.5cm} 3-gram & 0.62 & 0.73 & 0.52 \textcolor{gray}{(-15.6\%)} & 0.60 \textcolor{gray}{(-18.3\%)} & 0.60 \textcolor{gray}{(-4.0\%)} & 0.67 \textcolor{gray}{(-8.2\%)} \\
  Char 1-gram & 0.45 & 0.45 & 0.22 \textcolor{gray}{(-52.2\%)} & 0.20 \textcolor{gray}{(-56.2\%)} & 0.58 \textcolor{gray}{(+27.2\%)} & 0.58 \textcolor{gray}{(+29.6\%)} \\
  \hspace{0.5cm} 2-gram & 0.59 & 0.66 & 0.42 \textcolor{gray}{(-29.9\%)} & 0.43 \textcolor{gray}{(-33.9\%)} & 0.62 \textcolor{gray}{(+5.1\%)} & 0.68 \textcolor{gray}{(+3.8\%)} \\
  \hspace{0.5cm} 3-gram & 0.64 & 0.72 & 0.50 \textcolor{gray}{(-21.5\%)} & 0.56 \textcolor{gray}{(-23.2\%)} & 0.62 \textcolor{gray}{(-3.5\%)} & 0.68 \textcolor{gray}{(-5.7\%)} \\
  \hline
  \end{tabular}
\end{adjustbox}
  \end{center}
  \caption{\label{tab:stylo-feature-results-table} Results for the Vanilla Model and the Machine Translation Model. Numbers in the brackets are the differences in percentage compared to the corresponding untranslated results.}
\end{table*}

For the first two models I have applied grid search to find the best stylometric feature for my CLAA task. As we see in Table \ref{tab:stylo-feature-results-table}, the best features are usually the shorter word unigrams or bigrams for each classifier. Only in the translated Chinese group the best results appear in the char-level bigrams and trigrams. This exception can be explained by the difference of average word length. In Chinese the average word length is about one to two characters while in English it is about four to five letters \cite{chen2015does} \cite{bochkarev2015average}. In other words, Chinese characters them alone can carry as much information as English words. The result that word-level n-grams are more effective than char-level is aligned with results from many previous works in many other AA tasks \cite{kestemont2018overview} \cite{rangel2019overview}.

Also the SVM classifier outperformed the logistic regression classifier in nearly all comparisions, except in the char-level unigram one. This can be seen in \citet{rocha2016authorship}'s work as well. They attributed it to the application of Maxiumn Margin Princple in SVM classifiers. However for AA task on long articles, logistic regression could be better than SVM \cite{bogdanova2014cross}. Thus I think SVM is a better choice for short social media texts than logistic regression.

\subsection{Distortion from Machine Translation}

As mentioned previously, machine translation will inevitably introduce noise into the text and will bring down the accuracy. In Table \ref{tab:stylo-feature-results-table} I have also calculated the impact of machine translation by comparing them to the untranslated original text. Word-level bigrams have topped the chart in almost every group, followed closely by word-level trigrams and unigrams, which once again showed that word-level n-grams are better features than char-level n-grams in our bilingual tweet dataset. Further more, LIBLINEAR SVM classifier still achieved higher accuracy than the logistic regression classifier after machine translation, showed that it is more suitable for short social media text no matter of the text language.

Move on to the performance difference between the translated Chinese and English text, we can see that Chinese suffered more than English if it was translated. Especially in the case of char unigram, when all Chinese text are translated into English the performance dropped more than 50\%. In contrast while we turned all English content into Chinese, we have manageed to improve the performance by nearly 30\%. Since many Chinese characters can serve as a word alone, short char-level n-grams in Chinese can be viewed as word-level n-grams in English. Thus explains why we had a large gain on performance when we translate everything into Chinese.

\subsection{Aligned Word Embeddings}

\begin{table}[t]
  \begin{center}
  \begin{tabular}{|l|r|}
  \hline \bf Word Embeddings & \bf Accuracy \\ \hline
  Aligned Bilingual & 0.70 \\
  \hspace{0.5cm} ZH Only & 0.68 \\
  \hspace{0.5cm} EN Only & 0.69 \\
  Unaligned Bilingual & 0.65 \\
  \hline
  \end{tabular}
  \end{center}
  \caption{\label{tab:cnn-results-table} Results for the Aligned Word Embedding Model}
\end{table}

Finally the results for the Aligned Word Embedding Model are shown in Table \ref{tab:cnn-results-table}. I have also added a reference group using the unaligned common fastText embeddings. Both the aligned and the unaligned word embeddings for Chinese and English were pre-trained on Wikipedia text \cite{bojanowski2017enriching}.

The Aligned Word Embedding Model didn't surpass my Vanilla Model. However its accuracy was much closer to the Vanilla Model than the one from the Machine Translation Model. The performance gain by using bilingual embeddings was subtle compared to monolingual embeddings, only around 2\%. The unaligned model performed worst among these three kinds of embeddings as expected.

The result for the Aligned Word Embedding Model might suggest a better combination for bilingual word embeddings rather than concatenation. But there are other facts that could also affect the final result. First, the fastText word embeddings were trained on a domian that was far away from social media text. Wikipedia is a more formal, serious and comprehensive place than Twitter, and the topics it includes has little intersection with the topics from Twitter. The results could be improved by training a dedicated word embeddings from a Twitter corpus. Another thing to remember is the imbalance between Chinese and English word embedding sizes. The English embeddings has 2519370 items and is 7.5 times larger than the Chinese embeddings, just as English Wikipedia articles are around 5 times more than articles in Chinese \footnote{As Dec 2018, \url{https://stats.wikimedia.org/EN/TablesWikipediaEN.htm}} \footnote{As Dec 2018, \url{https://stats.wikimedia.org/EN/TablesWikipediaZH.htm}}. Smaller embeddings size will introduce more OOV words and will lower the overall accuracy.

\section{Conclusion}\label{sec:conclusion}

In this project I have explored the possibilities of three different models for a bilingual AA question. On a dataset collected from Twitter, the simple SVM classifier with word-level and char-level n-grams achieved the highest accuracy, followed by the Aligned Word Embedding Model. I have also discussed the distortion brought by machine translation. It turned out that even though the Aligned Word Embedding Model didn't give the best result, it has the potential to be one of the solutions to the CLAA problem as well.

In futher works, the emphisis should be on a more sophiscated assembling of bilingual word embeddings. In addition, world clusters \cite{tackstrom2012cross} is another promising model to transfer from one language to another. Either of them avoids the inevitable distortion from machine translation, hence they could be applied as new features for CLAA.

\bibliography{report}
\bibliographystyle{acl_natbib}

\end{document}


