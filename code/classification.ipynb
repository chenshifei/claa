{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 2,
    "file_extension": ".py",
    "mimetype": "text/x-python",
    "name": "python",
    "npconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "version": 3,
    "colab": {
      "name": "classification.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "J4hqhoKs_G-J",
        "YmZG99Al_G-M"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "dmmpF4VcJSvo",
        "outputId": "b312940c-5238-439f-af42-0b347680ddce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": "(69170, 7)"
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# df = pd.read_csv('drive/My Drive/tweets_monolingual_5p.csv')\n",
        "df = pd.read_csv('../data/tweets_monolingual_5p.csv')\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>id_str</th>\n      <th>screen_name</th>\n      <th>raw_text</th>\n      <th>chn_text</th>\n      <th>eng_text</th>\n      <th>monolingual_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1193152895337689088</td>\n      <td>YXSzzzz</td>\n      <td>@gww067 Haha there is no way Ryza is not cute 😂</td>\n      <td>NaN</td>\n      <td>Haha there is no way Ryza is not cute</td>\n      <td>Haha there is no way Ryza is not cute</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1193152895337689088</td>\n      <td>YXSzzzz</td>\n      <td>乳不平何以平天下!\\nLess busty testing. https://t.co/Iy...</td>\n      <td>乳不平何以平天下!</td>\n      <td>Less busty testing.</td>\n      <td>乳不平何以平天下!</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1193152895337689088</td>\n      <td>YXSzzzz</td>\n      <td>@HrJasn 乳不巨何以聚人心 (誤</td>\n      <td>乳不巨何以聚人心誤</td>\n      <td>NaN</td>\n      <td>乳不巨何以聚人心誤</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>1193152895337689088</td>\n      <td>YXSzzzz</td>\n      <td>因為蠻多人問。。。我就統一發個 😂\\nBecuz too many people ask ....</td>\n      <td>因為蠻多人問。。。我就統一發個這個是我自己無聊畫的，不是…</td>\n      <td>Becuz too many people ask ... I will post thi...</td>\n      <td>因為蠻多人問。。。我就統一發個這個是我自己無聊畫的，不是…</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>1193152895337689088</td>\n      <td>YXSzzzz</td>\n      <td>因為蠻多人問。。。我就統一發個 😂\\nBecuz too many people ask ....</td>\n      <td>因為蠻多人問。。。我就統一發個這個是我自己無聊畫的，不是…</td>\n      <td>Becuz too many people ask ... I will post thi...</td>\n      <td>Becuz too many people ask ... I will post thi...</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "   Unnamed: 0               id_str screen_name  \\\n0           0  1193152895337689088     YXSzzzz   \n1           1  1193152895337689088     YXSzzzz   \n2           3  1193152895337689088     YXSzzzz   \n3           4  1193152895337689088     YXSzzzz   \n4           5  1193152895337689088     YXSzzzz   \n\n                                            raw_text  \\\n0    @gww067 Haha there is no way Ryza is not cute 😂   \n1  乳不平何以平天下!\\nLess busty testing. https://t.co/Iy...   \n2                                @HrJasn 乳不巨何以聚人心 (誤   \n3  因為蠻多人問。。。我就統一發個 😂\\nBecuz too many people ask ....   \n4  因為蠻多人問。。。我就統一發個 😂\\nBecuz too many people ask ....   \n\n                        chn_text  \\\n0                            NaN   \n1                      乳不平何以平天下!   \n2                      乳不巨何以聚人心誤   \n3  因為蠻多人問。。。我就統一發個這個是我自己無聊畫的，不是…   \n4  因為蠻多人問。。。我就統一發個這個是我自己無聊畫的，不是…   \n\n                                            eng_text  \\\n0              Haha there is no way Ryza is not cute   \n1                                Less busty testing.   \n2                                                NaN   \n3   Becuz too many people ask ... I will post thi...   \n4   Becuz too many people ask ... I will post thi...   \n\n                                    monolingual_text  \n0              Haha there is no way Ryza is not cute  \n1                                          乳不平何以平天下!  \n2                                          乳不巨何以聚人心誤  \n3                      因為蠻多人問。。。我就統一發個這個是我自己無聊畫的，不是…  \n4   Becuz too many people ask ... I will post thi...  "
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.drop('Unnamed: 0', inline=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": "array(['YXSzzzz', 'GreenBright2', 'ZhuLiuHai1', 'SangPu15', 'BlessHK3',\n       '0318May', '2nmnmcvkhKwByww', 'RainbowRainRai1', 'kacavawu',\n       'Lucien86734903', 'yueming02341069', 'susanzh77455188',\n       'Abby23079586', 'CherieHuang3', 'yuenck3', 'hkunderwearmen',\n       'Iming9999', 'lee_tungtung', 'ZazousLes', 'Heerfei',\n       'AnnaRao32559926', 'amyfox2009', 'pyequality', 'dgchuanghe',\n       'EmmaHaoTheBest', 'kevin77757997', 'YingShi55', 'Wangcat11',\n       'KP_Taipei', 'Nan_Is_Me', 'kelvinchan4335', 'EdrWx',\n       'Shevonne1207', 'steffenroski', 'cla12a2013', 'bluerightking',\n       'thenapoleonhung', 'easoncxz', 'siumou', 'yipcw',\n       'SVWomanEngineer', 'kk23wong', 'calvynation', 'AndyCoach',\n       'denistsao', 'Leifson', 'hongliji', 'sunbear', 'TrueOfLifeTP',\n       'poiesis'], dtype=object)"
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.screen_name.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": "50"
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(df.screen_name.unique())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "C76FvFRO_G-B"
      },
      "source": [
        "## Vectorize the tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Requirement already satisfied: polyglot in /usr/local/lib/python3.7/site-packages (16.7.4)\nRequirement already satisfied: PyICU in /usr/local/lib/python3.7/site-packages (2.4.2)\nRequirement already satisfied: pycld2 in /usr/local/lib/python3.7/site-packages (0.41)\nRequirement already satisfied: morfessor in /usr/local/lib/python3.7/site-packages (2.0.6)\n"
        }
      ],
      "source": [
        "!pip3 install polyglot PyICU pycld2 morfessor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Oox_xcHP0a98"
      },
      "source": [
        "### Prepare Monolingual Texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>Unnamed: 0.1</th>\n      <th>id_str</th>\n      <th>screen_name</th>\n      <th>raw_text</th>\n      <th>chn_text</th>\n      <th>eng_text</th>\n      <th>translated_chn</th>\n      <th>monolingual_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1193152895337689088</td>\n      <td>YXSzzzz</td>\n      <td>@gww067 Haha there is no way Ryza is not cute 😂</td>\n      <td>NaN</td>\n      <td>Haha there is no way Ryza is not cute</td>\n      <td>哈哈，不可能瑞紮不可愛</td>\n      <td>Haha there is no way Ryza is not cute</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1193152895337689088</td>\n      <td>YXSzzzz</td>\n      <td>乳不平何以平天下!\\nLess busty testing. https://t.co/Iy...</td>\n      <td>乳不平何以平天下!</td>\n      <td>Less busty testing.</td>\n      <td>不太繁忙的測試。</td>\n      <td>乳不平何以平天下!</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1193152895337689088</td>\n      <td>YXSzzzz</td>\n      <td>乳不平何以平天下!\\nLess busty testing. https://t.co/Iy...</td>\n      <td>乳不平何以平天下!</td>\n      <td>Less busty testing.</td>\n      <td>不太繁忙的測試。</td>\n      <td>Less busty testing.</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2</td>\n      <td>2</td>\n      <td>1193152895337689088</td>\n      <td>YXSzzzz</td>\n      <td>@HrJasn 乳不巨何以聚人心 (誤</td>\n      <td>乳不巨何以聚人心誤</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>乳不巨何以聚人心誤</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3</td>\n      <td>3</td>\n      <td>1193152895337689088</td>\n      <td>YXSzzzz</td>\n      <td>因為蠻多人問。。。我就統一發個 😂\\nBecuz too many people ask ....</td>\n      <td>因為蠻多人問。。。我就統一發個這個是我自己無聊畫的，不是…</td>\n      <td>Becuz too many people ask ... I will post thi...</td>\n      <td>因為太多人問...我會張貼這個Mod這是我的圖紙，而fe</td>\n      <td>因為蠻多人問。。。我就統一發個這個是我自己無聊畫的，不是…</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "   Unnamed: 0  Unnamed: 0.1               id_str screen_name  \\\n0           0             0  1193152895337689088     YXSzzzz   \n1           1             1  1193152895337689088     YXSzzzz   \n2           1             1  1193152895337689088     YXSzzzz   \n3           2             2  1193152895337689088     YXSzzzz   \n4           3             3  1193152895337689088     YXSzzzz   \n\n                                            raw_text  \\\n0    @gww067 Haha there is no way Ryza is not cute 😂   \n1  乳不平何以平天下!\\nLess busty testing. https://t.co/Iy...   \n2  乳不平何以平天下!\\nLess busty testing. https://t.co/Iy...   \n3                                @HrJasn 乳不巨何以聚人心 (誤   \n4  因為蠻多人問。。。我就統一發個 😂\\nBecuz too many people ask ....   \n\n                        chn_text  \\\n0                            NaN   \n1                      乳不平何以平天下!   \n2                      乳不平何以平天下!   \n3                      乳不巨何以聚人心誤   \n4  因為蠻多人問。。。我就統一發個這個是我自己無聊畫的，不是…   \n\n                                            eng_text  \\\n0              Haha there is no way Ryza is not cute   \n1                                Less busty testing.   \n2                                Less busty testing.   \n3                                                NaN   \n4   Becuz too many people ask ... I will post thi...   \n\n                 translated_chn                       monolingual_text  \n0                   哈哈，不可能瑞紮不可愛  Haha there is no way Ryza is not cute  \n1                      不太繁忙的測試。                              乳不平何以平天下!  \n2                      不太繁忙的測試。                    Less busty testing.  \n3                           NaN                              乳不巨何以聚人心誤  \n4  因為太多人問...我會張貼這個Mod這是我的圖紙，而fe          因為蠻多人問。。。我就統一發個這個是我自己無聊畫的，不是…  "
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['monolingual_text'] = df[['chn_text', 'eng_text']].values.tolist()\n",
        "df = df.explode('monolingual_text')\n",
        "df = df.reset_index(drop=True)\n",
        "df = df.dropna(subset=['monolingual_text'])\n",
        "df = df.reset_index(drop=True)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": "(73860, 9)"
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "69sI2gt9-tkY"
      },
      "source": [
        "### Prepare Translated Monolingual Texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>id_str</th>\n      <th>screen_name</th>\n      <th>raw_text</th>\n      <th>chn_text</th>\n      <th>eng_text</th>\n      <th>translated_eng</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1193152895337689088</td>\n      <td>YXSzzzz</td>\n      <td>@gww067 Haha there is no way Ryza is not cute 😂</td>\n      <td>NaN</td>\n      <td>Haha there is no way Ryza is not cute</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1193152895337689088</td>\n      <td>YXSzzzz</td>\n      <td>乳不平何以平天下!\\nLess busty testing. https://t.co/Iy...</td>\n      <td>乳不平何以平天下!</td>\n      <td>Less busty testing.</td>\n      <td>Why is the milk uneven in the world!</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>1193152895337689088</td>\n      <td>YXSzzzz</td>\n      <td>@HrJasn 乳不巨何以聚人心 (誤</td>\n      <td>乳不巨何以聚人心誤</td>\n      <td>Why milk is not huge is the wrong thing to gat...</td>\n      <td>Why milk is not huge is the wrong thing to gat...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>1193152895337689088</td>\n      <td>YXSzzzz</td>\n      <td>因為蠻多人問。。。我就統一發個 😂\\nBecuz too many people ask ....</td>\n      <td>因為蠻多人問。。。我就統一發個這個是我自己無聊畫的，不是…</td>\n      <td>Becuz too many people ask ... I will post thi...</td>\n      <td>Because quite a lot of people ask... I'll send...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>1193152895337689088</td>\n      <td>YXSzzzz</td>\n      <td>PS: it was not my update. but good to see more...</td>\n      <td>NaN</td>\n      <td>PS: it was not my update. but good to see more...</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "   Unnamed: 0               id_str screen_name  \\\n0           0  1193152895337689088     YXSzzzz   \n1           1  1193152895337689088     YXSzzzz   \n2           2  1193152895337689088     YXSzzzz   \n3           3  1193152895337689088     YXSzzzz   \n4           4  1193152895337689088     YXSzzzz   \n\n                                            raw_text  \\\n0    @gww067 Haha there is no way Ryza is not cute 😂   \n1  乳不平何以平天下!\\nLess busty testing. https://t.co/Iy...   \n2                                @HrJasn 乳不巨何以聚人心 (誤   \n3  因為蠻多人問。。。我就統一發個 😂\\nBecuz too many people ask ....   \n4  PS: it was not my update. but good to see more...   \n\n                        chn_text  \\\n0                            NaN   \n1                      乳不平何以平天下!   \n2                      乳不巨何以聚人心誤   \n3  因為蠻多人問。。。我就統一發個這個是我自己無聊畫的，不是…   \n4                            NaN   \n\n                                            eng_text  \\\n0              Haha there is no way Ryza is not cute   \n1                                Less busty testing.   \n2  Why milk is not huge is the wrong thing to gat...   \n3   Becuz too many people ask ... I will post thi...   \n4  PS: it was not my update. but good to see more...   \n\n                                      translated_eng  \n0                                                NaN  \n1               Why is the milk uneven in the world!  \n2  Why milk is not huge is the wrong thing to gat...  \n3  Because quite a lot of people ask... I'll send...  \n4                                                NaN  "
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['eng_text'].fillna(df['translated_eng'], inplace=True)\n",
        "df.eng_text = df.eng_text.astype(str)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7H0h6cFZ1ho2"
      },
      "source": [
        "### Prepare X, y, CV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "from polyglot.text import Text\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "\n",
        "X = df.eng_text\n",
        "y = df.screen_name\n",
        "\n",
        "def ploy_tokenizer(raw_text):\n",
        "    return Text(raw_text).words\n",
        "\n",
        "count_vectorizer = CountVectorizer(tokenizer=ploy_tokenizer, max_features=5000)\n",
        "# tf_transformer = TfidfTransformer()\n",
        "\n",
        "kf = KFold(n_splits=10, shuffle=True, random_state=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YubBlf1d_G-F"
      },
      "source": [
        "## Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "import logging\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn import metrics\n",
        "\n",
        "logging.getLogger().setLevel(logging.ERROR)\n",
        "\n",
        "nb = MultinomialNB()\n",
        "lr = LogisticRegression(solver='liblinear', multi_class='auto', class_weight='balanced', verbose=1)\n",
        "svm = LinearSVC(class_weight='balanced', verbose=1)\n",
        "\n",
        "def tokenize_test_pipeline(model, dataset_X=X, dataset_y=y):\n",
        "    pipe = Pipeline([\n",
        "        ('count', count_vectorizer),\n",
        "        # ('tfidf', tf_transformer),\n",
        "        ('clf', globals()[model]),\n",
        "    ])\n",
        "\n",
        "    params = {\n",
        "        'count__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
        "        'count__analyzer': ['word', 'char'],\n",
        "    }\n",
        "\n",
        "    algorithm = ''\n",
        "    if model.startswith('lr'):\n",
        "        algorithm = 'Logistic Regression'\n",
        "    elif model.startswith('nb'):\n",
        "        algorithm = 'Multinomial Naive Bayes'\n",
        "    elif model.startswith('svm'):\n",
        "        algorithm = 'Linear SVM'\n",
        "\n",
        "    gd_clf = GridSearchCV(pipe, params, cv=10, scoring='accuracy', n_jobs=-1, verbose=1)\n",
        "    gd_clf.fit(dataset_X, dataset_y)\n",
        "\n",
        "    print(algorithm)\n",
        "    print('------------------')\n",
        "    print('Scores: ')\n",
        "    print(gd_clf.cv_results_)\n",
        "    print('Best params:')\n",
        "    print(gd_clf.best_params_)\n",
        "    print('Best score:')\n",
        "    print(gd_clf.best_score_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "J4hqhoKs_G-J"
      },
      "source": [
        "### Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed: 17.4min\n[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed: 62.7min finished\n[LibLinear]Logistic Regression\n------------------\nScores: \n{'mean_fit_time': array([132.14860823, 163.95448198, 210.12704864, 422.21671927,\n       934.13132191, 916.00238762]), 'std_fit_time': array([  5.64906842,   9.36070949,  18.60320582,  21.6396404 ,\n        25.21502694, 230.4503087 ]), 'mean_score_time': array([2.91949921, 5.36475735, 7.61303818, 0.51067312, 1.33316512,\n       1.50437529]), 'std_score_time': array([0.10039169, 0.29739901, 0.28535073, 0.04520666, 0.21666141,\n       0.47259682]), 'param_count__analyzer': masked_array(data=['word', 'word', 'word', 'char', 'char', 'char'],\n             mask=[False, False, False, False, False, False],\n       fill_value='?',\n            dtype=object), 'param_count__ngram_range': masked_array(data=[(1, 1), (1, 2), (1, 3), (1, 1), (1, 2), (1, 3)],\n             mask=[False, False, False, False, False, False],\n       fill_value='?',\n            dtype=object), 'params': [{'count__analyzer': 'word', 'count__ngram_range': (1, 1)}, {'count__analyzer': 'word', 'count__ngram_range': (1, 2)}, {'count__analyzer': 'word', 'count__ngram_range': (1, 3)}, {'count__analyzer': 'char', 'count__ngram_range': (1, 1)}, {'count__analyzer': 'char', 'count__ngram_range': (1, 2)}, {'count__analyzer': 'char', 'count__ngram_range': (1, 3)}], 'split0_test_score': array([0.61230329, 0.61758556, 0.61868603, 0.44965335, 0.61780566,\n       0.6406955 ]), 'split1_test_score': array([0.6611643 , 0.66039397, 0.66248487, 0.48794982, 0.66391548,\n       0.70067129]), 'split2_test_score': array([0.66094421, 0.66996809, 0.66611643, 0.49774403, 0.66501596,\n       0.70430285]), 'split3_test_score': array([0.67018818, 0.67668097, 0.67514031, 0.50357654, 0.68405414,\n       0.70749422]), 'split4_test_score': array([0.66941785, 0.67569055, 0.67679102, 0.50797843, 0.67436998,\n       0.70430285]), 'split5_test_score': array([0.65500165, 0.65863321, 0.65984373, 0.49180147, 0.665126  ,\n       0.69483878]), 'split6_test_score': array([0.68416419, 0.67811159, 0.6797623 , 0.51205018, 0.68966656,\n       0.71783867]), 'split7_test_score': array([0.66688676, 0.68042258, 0.67591064, 0.49499285, 0.68482447,\n       0.7123363 ]), 'split8_test_score': array([0.64729834, 0.65544184, 0.65059976, 0.46318917, 0.64333663,\n       0.68339386]), 'split9_test_score': array([0.58160009, 0.58248047, 0.58215032, 0.40409376, 0.5730164 ,\n       0.60911192]), 'mean_test_score': array([0.65089689, 0.65554088, 0.65474854, 0.48130296, 0.65611313,\n       0.68749862]), 'std_test_score': array([0.0293347 , 0.03000191, 0.02962849, 0.03171163, 0.03438215,\n       0.03328854]), 'rank_test_score': array([5, 3, 4, 6, 2, 1], dtype=int32)}\nBest params:\n{'count__analyzer': 'char', 'count__ngram_range': (1, 3)}\nBest score:\n0.6874986244084955\n/usr/local/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  \"the number of iterations.\", ConvergenceWarning)\n"
        }
      ],
      "source": [
        "tokenize_test_pipeline(model='lr')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YmZG99Al_G-M"
      },
      "source": [
        "### SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed: 32.6min\n[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed: 61.0min finished\n[LibLinear]Linear SVM\n------------------\nScores: \n{'mean_fit_time': array([ 195.10204113,  206.81252863,  237.98372262, 1101.0949352 ,\n        759.54852133,  314.18964348]), 'std_fit_time': array([ 6.44813057,  5.75966484,  3.55224823, 13.02302361,  8.46335838,\n       67.01068921]), 'mean_score_time': array([3.10376823, 5.34769273, 8.16282222, 0.61721988, 1.49795084,\n       1.78612366]), 'std_score_time': array([0.42781098, 0.74722465, 1.09875767, 0.02000622, 0.11999064,\n       0.61484092]), 'param_count__analyzer': masked_array(data=['word', 'word', 'word', 'char', 'char', 'char'],\n             mask=[False, False, False, False, False, False],\n       fill_value='?',\n            dtype=object), 'param_count__ngram_range': masked_array(data=[(1, 1), (1, 2), (1, 3), (1, 1), (1, 2), (1, 3)],\n             mask=[False, False, False, False, False, False],\n       fill_value='?',\n            dtype=object), 'params': [{'count__analyzer': 'word', 'count__ngram_range': (1, 1)}, {'count__analyzer': 'word', 'count__ngram_range': (1, 2)}, {'count__analyzer': 'word', 'count__ngram_range': (1, 3)}, {'count__analyzer': 'char', 'count__ngram_range': (1, 1)}, {'count__analyzer': 'char', 'count__ngram_range': (1, 2)}, {'count__analyzer': 'char', 'count__ngram_range': (1, 3)}], 'split0_test_score': array([0.57752834, 0.58919335, 0.58622208, 0.41003632, 0.57983933,\n       0.60338946]), 'split1_test_score': array([0.6344228 , 0.62770992, 0.62737977, 0.48189722, 0.64949928,\n       0.6653461 ]), 'split2_test_score': array([0.6283702 , 0.63376252, 0.63167162, 0.45801695, 0.62451854,\n       0.66897766]), 'split3_test_score': array([0.64443711, 0.64850886, 0.64509739, 0.48938043, 0.66545615,\n       0.66369539]), 'split4_test_score': array([0.63354242, 0.64718829, 0.64773853, 0.39286893, 0.6528007 ,\n       0.66776714]), 'split5_test_score': array([0.61637504, 0.62605921, 0.62495873, 0.39462969, 0.64344668,\n       0.65346099]), 'split6_test_score': array([0.64322659, 0.64454716, 0.64795862, 0.4785958 , 0.64355673,\n       0.67811159]), 'split7_test_score': array([0.63827446, 0.64234621, 0.63849455, 0.47254319, 0.66578629,\n       0.67580059]), 'split8_test_score': array([0.62044679, 0.62033674, 0.61857599, 0.42742379, 0.6223176 ,\n       0.64333663]), 'split9_test_score': array([0.5544184 , 0.5523275 , 0.55496864, 0.37052933, 0.53681083,\n       0.56509299]), 'mean_test_score': array([0.61910421, 0.62319798, 0.62230659, 0.43759216, 0.62840321,\n       0.64849785]), 'std_test_score': array([0.02835874, 0.02892386, 0.02838515, 0.04143802, 0.03881542,\n       0.03459824]), 'rank_test_score': array([5, 3, 4, 6, 2, 1], dtype=int32)}\nBest params:\n{'count__analyzer': 'char', 'count__ngram_range': (1, 3)}\nBest score:\n0.6484978540772532\n/usr/local/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  \"the number of iterations.\", ConvergenceWarning)\n"
        }
      ],
      "source": [
        "tokenize_test_pipeline(model='svm')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fnmjoafY_G-Q"
      },
      "source": [
        "### Translated ENG Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed: 12.3min\n[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed: 98.4min finished\n[LibLinear]Logistic Regression\n------------------\nScores: \n{'mean_fit_time': array([ 108.69338543,  142.32223682,  176.59184439,  181.75002394,\n       1579.79950054, 2035.36635232]), 'std_fit_time': array([  3.80517121,   2.89782448,   5.87818433,   9.42902107,\n        54.29913221, 520.90896675]), 'mean_score_time': array([2.51365199, 4.07062821, 5.77338424, 0.49230125, 1.19133818,\n       1.59018648]), 'std_score_time': array([0.26670071, 0.2429129 , 0.34958147, 0.0822156 , 0.06877389,\n       0.59974833]), 'param_count__analyzer': masked_array(data=['word', 'word', 'word', 'char', 'char', 'char'],\n             mask=[False, False, False, False, False, False],\n       fill_value='?',\n            dtype=object), 'param_count__ngram_range': masked_array(data=[(1, 1), (1, 2), (1, 3), (1, 1), (1, 2), (1, 3)],\n             mask=[False, False, False, False, False, False],\n       fill_value='?',\n            dtype=object), 'params': [{'count__analyzer': 'word', 'count__ngram_range': (1, 1)}, {'count__analyzer': 'word', 'count__ngram_range': (1, 2)}, {'count__analyzer': 'word', 'count__ngram_range': (1, 3)}, {'count__analyzer': 'char', 'count__ngram_range': (1, 1)}, {'count__analyzer': 'char', 'count__ngram_range': (1, 2)}, {'count__analyzer': 'char', 'count__ngram_range': (1, 3)}], 'split0_test_score': array([0.48803364, 0.50274903, 0.50582147, 0.19210867, 0.43078913,\n       0.52021345]), 'split1_test_score': array([0.55167394, 0.56202491, 0.55830503, 0.23047065, 0.49118551,\n       0.58596151]), 'split2_test_score': array([0.5591137 , 0.56574478, 0.55976063, 0.23920427, 0.49037684,\n       0.58515284]), 'split3_test_score': array([0.57625748, 0.57399321, 0.56946466, 0.24583536, 0.49312631,\n       0.58806405]), 'split4_test_score': array([0.56833252, 0.56978813, 0.5701116 , 0.23758693, 0.49603752,\n       0.5845059 ]), 'split5_test_score': array([0.58013909, 0.58515284, 0.58806405, 0.23613133, 0.51285784,\n       0.60633996]), 'split6_test_score': array([0.59809154, 0.59712114, 0.59340126, 0.23144105, 0.52531134,\n       0.61377972]), 'split7_test_score': array([0.58418244, 0.59469513, 0.59000485, 0.21737021, 0.51398997,\n       0.60941291]), 'split8_test_score': array([0.56024584, 0.5715672 , 0.57027333, 0.21041566, 0.47808507,\n       0.58677018]), 'split9_test_score': array([0.51965066, 0.51738638, 0.51334304, 0.20168203, 0.43603429,\n       0.53129549]), 'mean_test_score': array([0.55857208, 0.56402227, 0.56185499, 0.22422461, 0.48677938,\n       0.5811496 ]), 'std_test_score': array([0.0310356 , 0.02934063, 0.02860349, 0.01697085, 0.02974749,\n       0.02970957]), 'rank_test_score': array([4, 2, 3, 6, 5, 1], dtype=int32)}\nBest params:\n{'count__analyzer': 'char', 'count__ngram_range': (1, 3)}\nBest score:\n0.5811496002999502\n/usr/local/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  \"the number of iterations.\", ConvergenceWarning)\n"
        }
      ],
      "source": [
        "tokenize_test_pipeline(model='lr')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "44mb67f82CGD"
      },
      "source": [
        "### Translated ENG SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed: 33.3min\n[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed: 88.0min finished\n[LibLinear]Linear SVM\n------------------\nScores: \n{'mean_fit_time': array([ 199.86388612,  204.61199472,  208.59291446, 1194.05278988,\n       1540.43381255,  744.51242266]), 'std_fit_time': array([ 4.43497799,  4.7003657 ,  2.30412426, 30.52803481, 86.92077743,\n       99.0820755 ]), 'mean_score_time': array([2.19013524, 3.8466135 , 5.83341744, 0.46746995, 1.39227376,\n       2.02988892]), 'std_score_time': array([0.29277496, 0.38413285, 0.81796802, 0.01848559, 0.19368091,\n       0.78178129]), 'param_count__analyzer': masked_array(data=['word', 'word', 'word', 'char', 'char', 'char'],\n             mask=[False, False, False, False, False, False],\n       fill_value='?',\n            dtype=object), 'param_count__ngram_range': masked_array(data=[(1, 1), (1, 2), (1, 3), (1, 1), (1, 2), (1, 3)],\n             mask=[False, False, False, False, False, False],\n       fill_value='?',\n            dtype=object), 'params': [{'count__analyzer': 'word', 'count__ngram_range': (1, 1)}, {'count__analyzer': 'word', 'count__ngram_range': (1, 2)}, {'count__analyzer': 'word', 'count__ngram_range': (1, 3)}, {'count__analyzer': 'char', 'count__ngram_range': (1, 1)}, {'count__analyzer': 'char', 'count__ngram_range': (1, 2)}, {'count__analyzer': 'char', 'count__ngram_range': (1, 3)}], 'split0_test_score': array([0.46119017, 0.46377749, 0.46684994, 0.19615136, 0.42124838,\n       0.48803364]), 'split1_test_score': array([0.53016335, 0.53016335, 0.52531134, 0.22464823, 0.48115801,\n       0.55652596]), 'split2_test_score': array([0.52919295, 0.52822255, 0.52401747, 0.25521591, 0.48503962,\n       0.55118874]), 'split3_test_score': array([0.54213165, 0.53194242, 0.53194242, 0.24680576, 0.44476791,\n       0.54795407]), 'split4_test_score': array([0.54504286, 0.53889698, 0.5342067 , 0.22836811, 0.4873039 ,\n       0.55183568]), 'split5_test_score': array([0.54455766, 0.5481158 , 0.54973314, 0.18049491, 0.49765486,\n       0.56817079]), 'split6_test_score': array([0.56655345, 0.55620249, 0.55555556, 0.2249717 , 0.51738638,\n       0.58321203]), 'split7_test_score': array([0.55571729, 0.55895197, 0.55167394, 0.14685428, 0.50396248,\n       0.57092027]), 'split8_test_score': array([0.53194242, 0.53113375, 0.52822255, 0.16723273, 0.47032185,\n       0.54051431]), 'split9_test_score': array([0.49264111, 0.48762737, 0.47857027, 0.20232897, 0.43150574,\n       0.48940644]), 'mean_test_score': array([0.52991329, 0.52750342, 0.52460833, 0.20730719, 0.47403491,\n       0.54477619]), 'std_test_score': array([0.02952589, 0.0283882 , 0.02817249, 0.03322701, 0.03024287,\n       0.03041535]), 'rank_test_score': array([2, 3, 4, 6, 5, 1], dtype=int32)}\nBest params:\n{'count__analyzer': 'char', 'count__ngram_range': (1, 3)}\nBest score:\n0.5447761922426785\n/usr/local/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  \"the number of iterations.\", ConvergenceWarning)\n"
        }
      ],
      "source": [
        "tokenize_test_pipeline(model='svm')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EHQOSXNLKQEF"
      },
      "source": [
        "### Translated CHN Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed: 10.4min\n[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed: 22.5min finished\n[LibLinear]Logistic Regression\n------------------\nScores: \n{'mean_fit_time': array([ 79.85924022, 106.08262389, 142.9701786 , 201.78887875,\n       256.23497031, 229.90636399]), 'std_fit_time': array([ 1.91491963,  4.17896374, 13.28977827, 18.38440116,  7.02918066,\n       59.42484926]), 'mean_score_time': array([2.37760949, 3.90294809, 6.08079286, 0.34631639, 0.70903943,\n       0.72321827]), 'std_score_time': array([0.09669501, 0.14720386, 0.27737206, 0.03173112, 0.05290408,\n       0.24711447]), 'param_count__analyzer': masked_array(data=['word', 'word', 'word', 'char', 'char', 'char'],\n             mask=[False, False, False, False, False, False],\n       fill_value='?',\n            dtype=object), 'param_count__ngram_range': masked_array(data=[(1, 1), (1, 2), (1, 3), (1, 1), (1, 2), (1, 3)],\n             mask=[False, False, False, False, False, False],\n       fill_value='?',\n            dtype=object), 'params': [{'count__analyzer': 'word', 'count__ngram_range': (1, 1)}, {'count__analyzer': 'word', 'count__ngram_range': (1, 2)}, {'count__analyzer': 'word', 'count__ngram_range': (1, 3)}, {'count__analyzer': 'char', 'count__ngram_range': (1, 1)}, {'count__analyzer': 'char', 'count__ngram_range': (1, 2)}, {'count__analyzer': 'char', 'count__ngram_range': (1, 3)}], 'split0_test_score': array([0.5423674 , 0.54333765, 0.54333765, 0.51552393, 0.55336352,\n       0.54818887]), 'split1_test_score': array([0.60520783, 0.61135371, 0.61167718, 0.57965389, 0.61491186,\n       0.61911693]), 'split2_test_score': array([0.61313278, 0.62267508, 0.6204108 , 0.59178392, 0.6323791 ,\n       0.62833576]), 'split3_test_score': array([0.60375222, 0.61200065, 0.61086851, 0.58612324, 0.61976387,\n       0.61523532]), 'split4_test_score': array([0.61507359, 0.61669093, 0.61280932, 0.59420993, 0.62413068,\n       0.61701439]), 'split5_test_score': array([0.60278182, 0.6065017 , 0.60456089, 0.59049005, 0.62962963,\n       0.62914443]), 'split6_test_score': array([0.62865923, 0.62688016, 0.62267508, 0.61038331, 0.64370047,\n       0.63803979]), 'split7_test_score': array([0.62785056, 0.63334951, 0.63076177, 0.60488436, 0.64580301,\n       0.64483261]), 'split8_test_score': array([0.58191816, 0.58709364, 0.58369723, 0.55571729, 0.60359049,\n       0.59792981]), 'split9_test_score': array([0.54132298, 0.54423419, 0.54439592, 0.52353227, 0.56525958,\n       0.56137797]), 'mean_test_score': array([0.59620666, 0.60041172, 0.59851943, 0.57523022, 0.61325322,\n       0.60992159]), 'std_test_score': array([0.02998866, 0.03070505, 0.02977029, 0.03120693, 0.02960997,\n       0.03031884]), 'rank_test_score': array([5, 3, 4, 6, 1, 2], dtype=int32)}\nBest params:\n{'count__analyzer': 'char', 'count__ngram_range': (1, 2)}\nBest score:\n0.613253220709708\n/usr/local/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  \"the number of iterations.\", ConvergenceWarning)\n"
        }
      ],
      "source": [
        "tokenize_test_pipeline(model='lr')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cX4TjcQQKdPS"
      },
      "source": [
        "### Translated CHN SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\nFitting 10 folds for each of 6 candidates, totalling 60 fits\n[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed: 13.3min\n[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed: 19.6min finished\n[LibLinear]Linear SVM\n------------------\nScores: \n{'mean_fit_time': array([143.75854456, 154.623878  , 183.76461287, 157.76418879,\n       131.1755291 , 123.27293158]), 'std_fit_time': array([ 4.44951464,  2.62774771,  3.81153094,  3.69266955,  1.68336143,\n       22.46824012]), 'mean_score_time': array([2.87137532, 4.49891565, 6.832815  , 0.43380499, 0.8085367 ,\n       0.80856166]), 'std_score_time': array([0.19062099, 0.53707166, 0.88287584, 0.02238096, 0.05979435,\n       0.31407861]), 'param_count__analyzer': masked_array(data=['word', 'word', 'word', 'char', 'char', 'char'],\n             mask=[False, False, False, False, False, False],\n       fill_value='?',\n            dtype=object), 'param_count__ngram_range': masked_array(data=[(1, 1), (1, 2), (1, 3), (1, 1), (1, 2), (1, 3)],\n             mask=[False, False, False, False, False, False],\n       fill_value='?',\n            dtype=object), 'params': [{'count__analyzer': 'word', 'count__ngram_range': (1, 1)}, {'count__analyzer': 'word', 'count__ngram_range': (1, 2)}, {'count__analyzer': 'word', 'count__ngram_range': (1, 3)}, {'count__analyzer': 'char', 'count__ngram_range': (1, 1)}, {'count__analyzer': 'char', 'count__ngram_range': (1, 2)}, {'count__analyzer': 'char', 'count__ngram_range': (1, 3)}], 'split0_test_score': array([0.51115783, 0.50743855, 0.50663001, 0.47994825, 0.52457956,\n       0.52425614]), 'split1_test_score': array([0.56930293, 0.57561055, 0.57561055, 0.54552806, 0.57852175,\n       0.58013909]), 'split2_test_score': array([0.57269934, 0.58434417, 0.58078603, 0.55636422, 0.59518033,\n       0.59210739]), 'split3_test_score': array([0.56412745, 0.56234837, 0.56348051, 0.55296781, 0.57496361,\n       0.57949216]), 'split4_test_score': array([0.57172893, 0.58046256, 0.58013909, 0.56331878, 0.5849911 ,\n       0.58482937]), 'split5_test_score': array([0.56590652, 0.56477438, 0.56186317, 0.55490862, 0.58434417,\n       0.58887272]), 'split6_test_score': array([0.58709364, 0.58660844, 0.58935792, 0.57544881, 0.59712114,\n       0.59485687]), 'split7_test_score': array([0.58644671, 0.58757885, 0.58434417, 0.57318454, 0.60148795,\n       0.59776807]), 'split8_test_score': array([0.54180818, 0.54730713, 0.54245512, 0.51819505, 0.5595989 ,\n       0.54795407]), 'split9_test_score': array([0.501213  , 0.50008087, 0.50040433, 0.49199418, 0.51609251,\n       0.51593078]), 'mean_test_score': array([0.55714845, 0.55965539, 0.55850709, 0.54118583, 0.5716881 ,\n       0.57062067]), 'std_test_score': array([0.02821684, 0.03043827, 0.03039239, 0.03156349, 0.02817368,\n       0.02853911]), 'rank_test_score': array([5, 3, 4, 6, 1, 2], dtype=int32)}\nBest params:\n{'count__analyzer': 'char', 'count__ngram_range': (1, 2)}\nBest score:\n0.5716881031932693\n/usr/local/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  \"the number of iterations.\", ConvergenceWarning)\n"
        }
      ],
      "source": [
        "tokenize_test_pipeline(model='svm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ]
}