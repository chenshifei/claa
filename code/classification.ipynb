{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 2,
    "file_extension": ".py",
    "mimetype": "text/x-python",
    "name": "python",
    "npconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "version": 3,
    "colab": {
      "name": "classification.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "J4hqhoKs_G-J",
        "YmZG99Al_G-M"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "dmmpF4VcJSvo",
        "outputId": "b312940c-5238-439f-af42-0b347680ddce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": "(69170, 7)"
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# df = pd.read_csv('drive/My Drive/tweets_monolingual_5p.csv')\n",
        "df = pd.read_csv('../data/tweets_monolingual_5p.csv')\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>id_str</th>\n      <th>screen_name</th>\n      <th>raw_text</th>\n      <th>chn_text</th>\n      <th>eng_text</th>\n      <th>monolingual_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1193152895337689088</td>\n      <td>YXSzzzz</td>\n      <td>@gww067 Haha there is no way Ryza is not cute ğŸ˜‚</td>\n      <td>NaN</td>\n      <td>Haha there is no way Ryza is not cute</td>\n      <td>Haha there is no way Ryza is not cute</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1193152895337689088</td>\n      <td>YXSzzzz</td>\n      <td>ä¹³ä¸å¹³ä½•ä»¥å¹³å¤©ä¸‹!\\nLess busty testing. https://t.co/Iy...</td>\n      <td>ä¹³ä¸å¹³ä½•ä»¥å¹³å¤©ä¸‹!</td>\n      <td>Less busty testing.</td>\n      <td>ä¹³ä¸å¹³ä½•ä»¥å¹³å¤©ä¸‹!</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1193152895337689088</td>\n      <td>YXSzzzz</td>\n      <td>@HrJasn ä¹³ä¸å·¨ä½•ä»¥èšäººå¿ƒ (èª¤</td>\n      <td>ä¹³ä¸å·¨ä½•ä»¥èšäººå¿ƒèª¤</td>\n      <td>NaN</td>\n      <td>ä¹³ä¸å·¨ä½•ä»¥èšäººå¿ƒèª¤</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>1193152895337689088</td>\n      <td>YXSzzzz</td>\n      <td>å› ç‚ºè »å¤šäººå•ã€‚ã€‚ã€‚æˆ‘å°±çµ±ä¸€ç™¼å€‹ ğŸ˜‚\\nBecuz too many people ask ....</td>\n      <td>å› ç‚ºè »å¤šäººå•ã€‚ã€‚ã€‚æˆ‘å°±çµ±ä¸€ç™¼å€‹é€™å€‹æ˜¯æˆ‘è‡ªå·±ç„¡èŠç•«çš„ï¼Œä¸æ˜¯â€¦</td>\n      <td>Becuz too many people ask ... I will post thi...</td>\n      <td>å› ç‚ºè »å¤šäººå•ã€‚ã€‚ã€‚æˆ‘å°±çµ±ä¸€ç™¼å€‹é€™å€‹æ˜¯æˆ‘è‡ªå·±ç„¡èŠç•«çš„ï¼Œä¸æ˜¯â€¦</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>1193152895337689088</td>\n      <td>YXSzzzz</td>\n      <td>å› ç‚ºè »å¤šäººå•ã€‚ã€‚ã€‚æˆ‘å°±çµ±ä¸€ç™¼å€‹ ğŸ˜‚\\nBecuz too many people ask ....</td>\n      <td>å› ç‚ºè »å¤šäººå•ã€‚ã€‚ã€‚æˆ‘å°±çµ±ä¸€ç™¼å€‹é€™å€‹æ˜¯æˆ‘è‡ªå·±ç„¡èŠç•«çš„ï¼Œä¸æ˜¯â€¦</td>\n      <td>Becuz too many people ask ... I will post thi...</td>\n      <td>Becuz too many people ask ... I will post thi...</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "   Unnamed: 0               id_str screen_name  \\\n0           0  1193152895337689088     YXSzzzz   \n1           1  1193152895337689088     YXSzzzz   \n2           3  1193152895337689088     YXSzzzz   \n3           4  1193152895337689088     YXSzzzz   \n4           5  1193152895337689088     YXSzzzz   \n\n                                            raw_text  \\\n0    @gww067 Haha there is no way Ryza is not cute ğŸ˜‚   \n1  ä¹³ä¸å¹³ä½•ä»¥å¹³å¤©ä¸‹!\\nLess busty testing. https://t.co/Iy...   \n2                                @HrJasn ä¹³ä¸å·¨ä½•ä»¥èšäººå¿ƒ (èª¤   \n3  å› ç‚ºè »å¤šäººå•ã€‚ã€‚ã€‚æˆ‘å°±çµ±ä¸€ç™¼å€‹ ğŸ˜‚\\nBecuz too many people ask ....   \n4  å› ç‚ºè »å¤šäººå•ã€‚ã€‚ã€‚æˆ‘å°±çµ±ä¸€ç™¼å€‹ ğŸ˜‚\\nBecuz too many people ask ....   \n\n                        chn_text  \\\n0                            NaN   \n1                      ä¹³ä¸å¹³ä½•ä»¥å¹³å¤©ä¸‹!   \n2                      ä¹³ä¸å·¨ä½•ä»¥èšäººå¿ƒèª¤   \n3  å› ç‚ºè »å¤šäººå•ã€‚ã€‚ã€‚æˆ‘å°±çµ±ä¸€ç™¼å€‹é€™å€‹æ˜¯æˆ‘è‡ªå·±ç„¡èŠç•«çš„ï¼Œä¸æ˜¯â€¦   \n4  å› ç‚ºè »å¤šäººå•ã€‚ã€‚ã€‚æˆ‘å°±çµ±ä¸€ç™¼å€‹é€™å€‹æ˜¯æˆ‘è‡ªå·±ç„¡èŠç•«çš„ï¼Œä¸æ˜¯â€¦   \n\n                                            eng_text  \\\n0              Haha there is no way Ryza is not cute   \n1                                Less busty testing.   \n2                                                NaN   \n3   Becuz too many people ask ... I will post thi...   \n4   Becuz too many people ask ... I will post thi...   \n\n                                    monolingual_text  \n0              Haha there is no way Ryza is not cute  \n1                                          ä¹³ä¸å¹³ä½•ä»¥å¹³å¤©ä¸‹!  \n2                                          ä¹³ä¸å·¨ä½•ä»¥èšäººå¿ƒèª¤  \n3                      å› ç‚ºè »å¤šäººå•ã€‚ã€‚ã€‚æˆ‘å°±çµ±ä¸€ç™¼å€‹é€™å€‹æ˜¯æˆ‘è‡ªå·±ç„¡èŠç•«çš„ï¼Œä¸æ˜¯â€¦  \n4   Becuz too many people ask ... I will post thi...  "
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.drop('Unnamed: 0', inline=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": "array(['YXSzzzz', 'GreenBright2', 'ZhuLiuHai1', 'SangPu15', 'BlessHK3',\n       '0318May', '2nmnmcvkhKwByww', 'RainbowRainRai1', 'kacavawu',\n       'Lucien86734903', 'yueming02341069', 'susanzh77455188',\n       'Abby23079586', 'CherieHuang3', 'yuenck3', 'hkunderwearmen',\n       'Iming9999', 'lee_tungtung', 'ZazousLes', 'Heerfei',\n       'AnnaRao32559926', 'amyfox2009', 'pyequality', 'dgchuanghe',\n       'EmmaHaoTheBest', 'kevin77757997', 'YingShi55', 'Wangcat11',\n       'KP_Taipei', 'Nan_Is_Me', 'kelvinchan4335', 'EdrWx',\n       'Shevonne1207', 'steffenroski', 'cla12a2013', 'bluerightking',\n       'thenapoleonhung', 'easoncxz', 'siumou', 'yipcw',\n       'SVWomanEngineer', 'kk23wong', 'calvynation', 'AndyCoach',\n       'denistsao', 'Leifson', 'hongliji', 'sunbear', 'TrueOfLifeTP',\n       'poiesis'], dtype=object)"
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.screen_name.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": "50"
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(df.screen_name.unique())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "C76FvFRO_G-B"
      },
      "source": [
        "## Vectorize the tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Requirement already satisfied: polyglot in /usr/local/lib/python3.7/site-packages (16.7.4)\nRequirement already satisfied: PyICU in /usr/local/lib/python3.7/site-packages (2.4.2)\nRequirement already satisfied: pycld2 in /usr/local/lib/python3.7/site-packages (0.41)\nRequirement already satisfied: morfessor in /usr/local/lib/python3.7/site-packages (2.0.6)\n"
        }
      ],
      "source": [
        "!pip3 install polyglot PyICU pycld2 morfessor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Oox_xcHP0a98"
      },
      "source": [
        "### Prepare Monolingual Texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>Unnamed: 0.1</th>\n      <th>id_str</th>\n      <th>screen_name</th>\n      <th>raw_text</th>\n      <th>chn_text</th>\n      <th>eng_text</th>\n      <th>translated_chn</th>\n      <th>monolingual_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1193152895337689088</td>\n      <td>YXSzzzz</td>\n      <td>@gww067 Haha there is no way Ryza is not cute ğŸ˜‚</td>\n      <td>NaN</td>\n      <td>Haha there is no way Ryza is not cute</td>\n      <td>å“ˆå“ˆï¼Œä¸å¯èƒ½ç‘ç´®ä¸å¯æ„›</td>\n      <td>Haha there is no way Ryza is not cute</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1193152895337689088</td>\n      <td>YXSzzzz</td>\n      <td>ä¹³ä¸å¹³ä½•ä»¥å¹³å¤©ä¸‹!\\nLess busty testing. https://t.co/Iy...</td>\n      <td>ä¹³ä¸å¹³ä½•ä»¥å¹³å¤©ä¸‹!</td>\n      <td>Less busty testing.</td>\n      <td>ä¸å¤ªç¹å¿™çš„æ¸¬è©¦ã€‚</td>\n      <td>ä¹³ä¸å¹³ä½•ä»¥å¹³å¤©ä¸‹!</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1193152895337689088</td>\n      <td>YXSzzzz</td>\n      <td>ä¹³ä¸å¹³ä½•ä»¥å¹³å¤©ä¸‹!\\nLess busty testing. https://t.co/Iy...</td>\n      <td>ä¹³ä¸å¹³ä½•ä»¥å¹³å¤©ä¸‹!</td>\n      <td>Less busty testing.</td>\n      <td>ä¸å¤ªç¹å¿™çš„æ¸¬è©¦ã€‚</td>\n      <td>Less busty testing.</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2</td>\n      <td>2</td>\n      <td>1193152895337689088</td>\n      <td>YXSzzzz</td>\n      <td>@HrJasn ä¹³ä¸å·¨ä½•ä»¥èšäººå¿ƒ (èª¤</td>\n      <td>ä¹³ä¸å·¨ä½•ä»¥èšäººå¿ƒèª¤</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>ä¹³ä¸å·¨ä½•ä»¥èšäººå¿ƒèª¤</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3</td>\n      <td>3</td>\n      <td>1193152895337689088</td>\n      <td>YXSzzzz</td>\n      <td>å› ç‚ºè »å¤šäººå•ã€‚ã€‚ã€‚æˆ‘å°±çµ±ä¸€ç™¼å€‹ ğŸ˜‚\\nBecuz too many people ask ....</td>\n      <td>å› ç‚ºè »å¤šäººå•ã€‚ã€‚ã€‚æˆ‘å°±çµ±ä¸€ç™¼å€‹é€™å€‹æ˜¯æˆ‘è‡ªå·±ç„¡èŠç•«çš„ï¼Œä¸æ˜¯â€¦</td>\n      <td>Becuz too many people ask ... I will post thi...</td>\n      <td>å› ç‚ºå¤ªå¤šäººå•...æˆ‘æœƒå¼µè²¼é€™å€‹Modé€™æ˜¯æˆ‘çš„åœ–ç´™ï¼Œè€Œfe</td>\n      <td>å› ç‚ºè »å¤šäººå•ã€‚ã€‚ã€‚æˆ‘å°±çµ±ä¸€ç™¼å€‹é€™å€‹æ˜¯æˆ‘è‡ªå·±ç„¡èŠç•«çš„ï¼Œä¸æ˜¯â€¦</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "   Unnamed: 0  Unnamed: 0.1               id_str screen_name  \\\n0           0             0  1193152895337689088     YXSzzzz   \n1           1             1  1193152895337689088     YXSzzzz   \n2           1             1  1193152895337689088     YXSzzzz   \n3           2             2  1193152895337689088     YXSzzzz   \n4           3             3  1193152895337689088     YXSzzzz   \n\n                                            raw_text  \\\n0    @gww067 Haha there is no way Ryza is not cute ğŸ˜‚   \n1  ä¹³ä¸å¹³ä½•ä»¥å¹³å¤©ä¸‹!\\nLess busty testing. https://t.co/Iy...   \n2  ä¹³ä¸å¹³ä½•ä»¥å¹³å¤©ä¸‹!\\nLess busty testing. https://t.co/Iy...   \n3                                @HrJasn ä¹³ä¸å·¨ä½•ä»¥èšäººå¿ƒ (èª¤   \n4  å› ç‚ºè »å¤šäººå•ã€‚ã€‚ã€‚æˆ‘å°±çµ±ä¸€ç™¼å€‹ ğŸ˜‚\\nBecuz too many people ask ....   \n\n                        chn_text  \\\n0                            NaN   \n1                      ä¹³ä¸å¹³ä½•ä»¥å¹³å¤©ä¸‹!   \n2                      ä¹³ä¸å¹³ä½•ä»¥å¹³å¤©ä¸‹!   \n3                      ä¹³ä¸å·¨ä½•ä»¥èšäººå¿ƒèª¤   \n4  å› ç‚ºè »å¤šäººå•ã€‚ã€‚ã€‚æˆ‘å°±çµ±ä¸€ç™¼å€‹é€™å€‹æ˜¯æˆ‘è‡ªå·±ç„¡èŠç•«çš„ï¼Œä¸æ˜¯â€¦   \n\n                                            eng_text  \\\n0              Haha there is no way Ryza is not cute   \n1                                Less busty testing.   \n2                                Less busty testing.   \n3                                                NaN   \n4   Becuz too many people ask ... I will post thi...   \n\n                 translated_chn                       monolingual_text  \n0                   å“ˆå“ˆï¼Œä¸å¯èƒ½ç‘ç´®ä¸å¯æ„›  Haha there is no way Ryza is not cute  \n1                      ä¸å¤ªç¹å¿™çš„æ¸¬è©¦ã€‚                              ä¹³ä¸å¹³ä½•ä»¥å¹³å¤©ä¸‹!  \n2                      ä¸å¤ªç¹å¿™çš„æ¸¬è©¦ã€‚                    Less busty testing.  \n3                           NaN                              ä¹³ä¸å·¨ä½•ä»¥èšäººå¿ƒèª¤  \n4  å› ç‚ºå¤ªå¤šäººå•...æˆ‘æœƒå¼µè²¼é€™å€‹Modé€™æ˜¯æˆ‘çš„åœ–ç´™ï¼Œè€Œfe          å› ç‚ºè »å¤šäººå•ã€‚ã€‚ã€‚æˆ‘å°±çµ±ä¸€ç™¼å€‹é€™å€‹æ˜¯æˆ‘è‡ªå·±ç„¡èŠç•«çš„ï¼Œä¸æ˜¯â€¦  "
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['monolingual_text'] = df[['chn_text', 'eng_text']].values.tolist()\n",
        "df = df.explode('monolingual_text')\n",
        "df = df.reset_index(drop=True)\n",
        "df = df.dropna(subset=['monolingual_text'])\n",
        "df = df.reset_index(drop=True)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": "(73860, 9)"
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "69sI2gt9-tkY"
      },
      "source": [
        "### Prepare Translated Monolingual Texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>id_str</th>\n      <th>screen_name</th>\n      <th>raw_text</th>\n      <th>chn_text</th>\n      <th>eng_text</th>\n      <th>translated_eng</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1193152895337689088</td>\n      <td>YXSzzzz</td>\n      <td>@gww067 Haha there is no way Ryza is not cute ğŸ˜‚</td>\n      <td>NaN</td>\n      <td>Haha there is no way Ryza is not cute</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1193152895337689088</td>\n      <td>YXSzzzz</td>\n      <td>ä¹³ä¸å¹³ä½•ä»¥å¹³å¤©ä¸‹!\\nLess busty testing. https://t.co/Iy...</td>\n      <td>ä¹³ä¸å¹³ä½•ä»¥å¹³å¤©ä¸‹!</td>\n      <td>Less busty testing.</td>\n      <td>Why is the milk uneven in the world!</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>1193152895337689088</td>\n      <td>YXSzzzz</td>\n      <td>@HrJasn ä¹³ä¸å·¨ä½•ä»¥èšäººå¿ƒ (èª¤</td>\n      <td>ä¹³ä¸å·¨ä½•ä»¥èšäººå¿ƒèª¤</td>\n      <td>Why milk is not huge is the wrong thing to gat...</td>\n      <td>Why milk is not huge is the wrong thing to gat...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>1193152895337689088</td>\n      <td>YXSzzzz</td>\n      <td>å› ç‚ºè »å¤šäººå•ã€‚ã€‚ã€‚æˆ‘å°±çµ±ä¸€ç™¼å€‹ ğŸ˜‚\\nBecuz too many people ask ....</td>\n      <td>å› ç‚ºè »å¤šäººå•ã€‚ã€‚ã€‚æˆ‘å°±çµ±ä¸€ç™¼å€‹é€™å€‹æ˜¯æˆ‘è‡ªå·±ç„¡èŠç•«çš„ï¼Œä¸æ˜¯â€¦</td>\n      <td>Becuz too many people ask ... I will post thi...</td>\n      <td>Because quite a lot of people ask... I'll send...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>1193152895337689088</td>\n      <td>YXSzzzz</td>\n      <td>PS: it was not my update. but good to see more...</td>\n      <td>NaN</td>\n      <td>PS: it was not my update. but good to see more...</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "   Unnamed: 0               id_str screen_name  \\\n0           0  1193152895337689088     YXSzzzz   \n1           1  1193152895337689088     YXSzzzz   \n2           2  1193152895337689088     YXSzzzz   \n3           3  1193152895337689088     YXSzzzz   \n4           4  1193152895337689088     YXSzzzz   \n\n                                            raw_text  \\\n0    @gww067 Haha there is no way Ryza is not cute ğŸ˜‚   \n1  ä¹³ä¸å¹³ä½•ä»¥å¹³å¤©ä¸‹!\\nLess busty testing. https://t.co/Iy...   \n2                                @HrJasn ä¹³ä¸å·¨ä½•ä»¥èšäººå¿ƒ (èª¤   \n3  å› ç‚ºè »å¤šäººå•ã€‚ã€‚ã€‚æˆ‘å°±çµ±ä¸€ç™¼å€‹ ğŸ˜‚\\nBecuz too many people ask ....   \n4  PS: it was not my update. but good to see more...   \n\n                        chn_text  \\\n0                            NaN   \n1                      ä¹³ä¸å¹³ä½•ä»¥å¹³å¤©ä¸‹!   \n2                      ä¹³ä¸å·¨ä½•ä»¥èšäººå¿ƒèª¤   \n3  å› ç‚ºè »å¤šäººå•ã€‚ã€‚ã€‚æˆ‘å°±çµ±ä¸€ç™¼å€‹é€™å€‹æ˜¯æˆ‘è‡ªå·±ç„¡èŠç•«çš„ï¼Œä¸æ˜¯â€¦   \n4                            NaN   \n\n                                            eng_text  \\\n0              Haha there is no way Ryza is not cute   \n1                                Less busty testing.   \n2  Why milk is not huge is the wrong thing to gat...   \n3   Becuz too many people ask ... I will post thi...   \n4  PS: it was not my update. but good to see more...   \n\n                                      translated_eng  \n0                                                NaN  \n1               Why is the milk uneven in the world!  \n2  Why milk is not huge is the wrong thing to gat...  \n3  Because quite a lot of people ask... I'll send...  \n4                                                NaN  "
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['eng_text'].fillna(df['translated_eng'], inplace=True)\n",
        "df.eng_text = df.eng_text.astype(str)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7H0h6cFZ1ho2"
      },
      "source": [
        "### Prepare X, y, CV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "from polyglot.text import Text\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "\n",
        "X = df.eng_text\n",
        "y = df.screen_name\n",
        "\n",
        "def ploy_tokenizer(raw_text):\n",
        "    return Text(raw_text).words\n",
        "\n",
        "count_vectorizer = CountVectorizer(tokenizer=ploy_tokenizer, max_features=5000)\n",
        "# tf_transformer = TfidfTransformer()\n",
        "\n",
        "kf = KFold(n_splits=10, shuffle=True, random_state=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YubBlf1d_G-F"
      },
      "source": [
        "## Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "import logging\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn import metrics\n",
        "\n",
        "logging.getLogger().setLevel(logging.ERROR)\n",
        "\n",
        "nb = MultinomialNB()\n",
        "lr = LogisticRegression(solver='liblinear', multi_class='auto', class_weight='balanced', verbose=1)\n",
        "svm = LinearSVC(class_weight='balanced', verbose=1)\n",
        "\n",
        "def tokenize_test_pipeline(model, dataset_X=X, dataset_y=y):\n",
        "    pipe = Pipeline([\n",
        "        ('count', count_vectorizer),\n",
        "        # ('tfidf', tf_transformer),\n",
        "        ('clf', globals()[model]),\n",
        "    ])\n",
        "\n",
        "    params = {\n",
        "        'count__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
        "        'count__analyzer': ['word', 'char'],\n",
        "    }\n",
        "\n",
        "    algorithm = ''\n",
        "    if model.startswith('lr'):\n",
        "        algorithm = 'Logistic Regression'\n",
        "    elif model.startswith('nb'):\n",
        "        algorithm = 'Multinomial Naive Bayes'\n",
        "    elif model.startswith('svm'):\n",
        "        algorithm = 'Linear SVM'\n",
        "\n",
        "    gd_clf = GridSearchCV(pipe, params, cv=10, scoring='accuracy', n_jobs=-1, verbose=1)\n",
        "    gd_clf.fit(dataset_X, dataset_y)\n",
        "\n",
        "    print(algorithm)\n",
        "    print('------------------')\n",
        "    print('Scores: ')\n",
        "    print(gd_clf.cv_results_)\n",
        "    print('Best params:')\n",
        "    print(gd_clf.best_params_)\n",
        "    print('Best score:')\n",
        "    print(gd_clf.best_score_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "J4hqhoKs_G-J"
      },
      "source": [
        "### Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed: 17.4min\n[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed: 62.7min finished\n[LibLinear]Logistic Regression\n------------------\nScores: \n{'mean_fit_time': array([132.14860823, 163.95448198, 210.12704864, 422.21671927,\n       934.13132191, 916.00238762]), 'std_fit_time': array([  5.64906842,   9.36070949,  18.60320582,  21.6396404 ,\n        25.21502694, 230.4503087 ]), 'mean_score_time': array([2.91949921, 5.36475735, 7.61303818, 0.51067312, 1.33316512,\n       1.50437529]), 'std_score_time': array([0.10039169, 0.29739901, 0.28535073, 0.04520666, 0.21666141,\n       0.47259682]), 'param_count__analyzer': masked_array(data=['word', 'word', 'word', 'char', 'char', 'char'],\n             mask=[False, False, False, False, False, False],\n       fill_value='?',\n            dtype=object), 'param_count__ngram_range': masked_array(data=[(1, 1), (1, 2), (1, 3), (1, 1), (1, 2), (1, 3)],\n             mask=[False, False, False, False, False, False],\n       fill_value='?',\n            dtype=object), 'params': [{'count__analyzer': 'word', 'count__ngram_range': (1, 1)}, {'count__analyzer': 'word', 'count__ngram_range': (1, 2)}, {'count__analyzer': 'word', 'count__ngram_range': (1, 3)}, {'count__analyzer': 'char', 'count__ngram_range': (1, 1)}, {'count__analyzer': 'char', 'count__ngram_range': (1, 2)}, {'count__analyzer': 'char', 'count__ngram_range': (1, 3)}], 'split0_test_score': array([0.61230329, 0.61758556, 0.61868603, 0.44965335, 0.61780566,\n       0.6406955 ]), 'split1_test_score': array([0.6611643 , 0.66039397, 0.66248487, 0.48794982, 0.66391548,\n       0.70067129]), 'split2_test_score': array([0.66094421, 0.66996809, 0.66611643, 0.49774403, 0.66501596,\n       0.70430285]), 'split3_test_score': array([0.67018818, 0.67668097, 0.67514031, 0.50357654, 0.68405414,\n       0.70749422]), 'split4_test_score': array([0.66941785, 0.67569055, 0.67679102, 0.50797843, 0.67436998,\n       0.70430285]), 'split5_test_score': array([0.65500165, 0.65863321, 0.65984373, 0.49180147, 0.665126  ,\n       0.69483878]), 'split6_test_score': array([0.68416419, 0.67811159, 0.6797623 , 0.51205018, 0.68966656,\n       0.71783867]), 'split7_test_score': array([0.66688676, 0.68042258, 0.67591064, 0.49499285, 0.68482447,\n       0.7123363 ]), 'split8_test_score': array([0.64729834, 0.65544184, 0.65059976, 0.46318917, 0.64333663,\n       0.68339386]), 'split9_test_score': array([0.58160009, 0.58248047, 0.58215032, 0.40409376, 0.5730164 ,\n       0.60911192]), 'mean_test_score': array([0.65089689, 0.65554088, 0.65474854, 0.48130296, 0.65611313,\n       0.68749862]), 'std_test_score': array([0.0293347 , 0.03000191, 0.02962849, 0.03171163, 0.03438215,\n       0.03328854]), 'rank_test_score': array([5, 3, 4, 6, 2, 1], dtype=int32)}\nBest params:\n{'count__analyzer': 'char', 'count__ngram_range': (1, 3)}\nBest score:\n0.6874986244084955\n/usr/local/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  \"the number of iterations.\", ConvergenceWarning)\n"
        }
      ],
      "source": [
        "tokenize_test_pipeline(model='lr')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YmZG99Al_G-M"
      },
      "source": [
        "### SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed: 32.6min\n[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed: 61.0min finished\n[LibLinear]Linear SVM\n------------------\nScores: \n{'mean_fit_time': array([ 195.10204113,  206.81252863,  237.98372262, 1101.0949352 ,\n        759.54852133,  314.18964348]), 'std_fit_time': array([ 6.44813057,  5.75966484,  3.55224823, 13.02302361,  8.46335838,\n       67.01068921]), 'mean_score_time': array([3.10376823, 5.34769273, 8.16282222, 0.61721988, 1.49795084,\n       1.78612366]), 'std_score_time': array([0.42781098, 0.74722465, 1.09875767, 0.02000622, 0.11999064,\n       0.61484092]), 'param_count__analyzer': masked_array(data=['word', 'word', 'word', 'char', 'char', 'char'],\n             mask=[False, False, False, False, False, False],\n       fill_value='?',\n            dtype=object), 'param_count__ngram_range': masked_array(data=[(1, 1), (1, 2), (1, 3), (1, 1), (1, 2), (1, 3)],\n             mask=[False, False, False, False, False, False],\n       fill_value='?',\n            dtype=object), 'params': [{'count__analyzer': 'word', 'count__ngram_range': (1, 1)}, {'count__analyzer': 'word', 'count__ngram_range': (1, 2)}, {'count__analyzer': 'word', 'count__ngram_range': (1, 3)}, {'count__analyzer': 'char', 'count__ngram_range': (1, 1)}, {'count__analyzer': 'char', 'count__ngram_range': (1, 2)}, {'count__analyzer': 'char', 'count__ngram_range': (1, 3)}], 'split0_test_score': array([0.57752834, 0.58919335, 0.58622208, 0.41003632, 0.57983933,\n       0.60338946]), 'split1_test_score': array([0.6344228 , 0.62770992, 0.62737977, 0.48189722, 0.64949928,\n       0.6653461 ]), 'split2_test_score': array([0.6283702 , 0.63376252, 0.63167162, 0.45801695, 0.62451854,\n       0.66897766]), 'split3_test_score': array([0.64443711, 0.64850886, 0.64509739, 0.48938043, 0.66545615,\n       0.66369539]), 'split4_test_score': array([0.63354242, 0.64718829, 0.64773853, 0.39286893, 0.6528007 ,\n       0.66776714]), 'split5_test_score': array([0.61637504, 0.62605921, 0.62495873, 0.39462969, 0.64344668,\n       0.65346099]), 'split6_test_score': array([0.64322659, 0.64454716, 0.64795862, 0.4785958 , 0.64355673,\n       0.67811159]), 'split7_test_score': array([0.63827446, 0.64234621, 0.63849455, 0.47254319, 0.66578629,\n       0.67580059]), 'split8_test_score': array([0.62044679, 0.62033674, 0.61857599, 0.42742379, 0.6223176 ,\n       0.64333663]), 'split9_test_score': array([0.5544184 , 0.5523275 , 0.55496864, 0.37052933, 0.53681083,\n       0.56509299]), 'mean_test_score': array([0.61910421, 0.62319798, 0.62230659, 0.43759216, 0.62840321,\n       0.64849785]), 'std_test_score': array([0.02835874, 0.02892386, 0.02838515, 0.04143802, 0.03881542,\n       0.03459824]), 'rank_test_score': array([5, 3, 4, 6, 2, 1], dtype=int32)}\nBest params:\n{'count__analyzer': 'char', 'count__ngram_range': (1, 3)}\nBest score:\n0.6484978540772532\n/usr/local/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  \"the number of iterations.\", ConvergenceWarning)\n"
        }
      ],
      "source": [
        "tokenize_test_pipeline(model='svm')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fnmjoafY_G-Q"
      },
      "source": [
        "### Translated ENG Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed: 12.3min\n[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed: 98.4min finished\n[LibLinear]Logistic Regression\n------------------\nScores: \n{'mean_fit_time': array([ 108.69338543,  142.32223682,  176.59184439,  181.75002394,\n       1579.79950054, 2035.36635232]), 'std_fit_time': array([  3.80517121,   2.89782448,   5.87818433,   9.42902107,\n        54.29913221, 520.90896675]), 'mean_score_time': array([2.51365199, 4.07062821, 5.77338424, 0.49230125, 1.19133818,\n       1.59018648]), 'std_score_time': array([0.26670071, 0.2429129 , 0.34958147, 0.0822156 , 0.06877389,\n       0.59974833]), 'param_count__analyzer': masked_array(data=['word', 'word', 'word', 'char', 'char', 'char'],\n             mask=[False, False, False, False, False, False],\n       fill_value='?',\n            dtype=object), 'param_count__ngram_range': masked_array(data=[(1, 1), (1, 2), (1, 3), (1, 1), (1, 2), (1, 3)],\n             mask=[False, False, False, False, False, False],\n       fill_value='?',\n            dtype=object), 'params': [{'count__analyzer': 'word', 'count__ngram_range': (1, 1)}, {'count__analyzer': 'word', 'count__ngram_range': (1, 2)}, {'count__analyzer': 'word', 'count__ngram_range': (1, 3)}, {'count__analyzer': 'char', 'count__ngram_range': (1, 1)}, {'count__analyzer': 'char', 'count__ngram_range': (1, 2)}, {'count__analyzer': 'char', 'count__ngram_range': (1, 3)}], 'split0_test_score': array([0.48803364, 0.50274903, 0.50582147, 0.19210867, 0.43078913,\n       0.52021345]), 'split1_test_score': array([0.55167394, 0.56202491, 0.55830503, 0.23047065, 0.49118551,\n       0.58596151]), 'split2_test_score': array([0.5591137 , 0.56574478, 0.55976063, 0.23920427, 0.49037684,\n       0.58515284]), 'split3_test_score': array([0.57625748, 0.57399321, 0.56946466, 0.24583536, 0.49312631,\n       0.58806405]), 'split4_test_score': array([0.56833252, 0.56978813, 0.5701116 , 0.23758693, 0.49603752,\n       0.5845059 ]), 'split5_test_score': array([0.58013909, 0.58515284, 0.58806405, 0.23613133, 0.51285784,\n       0.60633996]), 'split6_test_score': array([0.59809154, 0.59712114, 0.59340126, 0.23144105, 0.52531134,\n       0.61377972]), 'split7_test_score': array([0.58418244, 0.59469513, 0.59000485, 0.21737021, 0.51398997,\n       0.60941291]), 'split8_test_score': array([0.56024584, 0.5715672 , 0.57027333, 0.21041566, 0.47808507,\n       0.58677018]), 'split9_test_score': array([0.51965066, 0.51738638, 0.51334304, 0.20168203, 0.43603429,\n       0.53129549]), 'mean_test_score': array([0.55857208, 0.56402227, 0.56185499, 0.22422461, 0.48677938,\n       0.5811496 ]), 'std_test_score': array([0.0310356 , 0.02934063, 0.02860349, 0.01697085, 0.02974749,\n       0.02970957]), 'rank_test_score': array([4, 2, 3, 6, 5, 1], dtype=int32)}\nBest params:\n{'count__analyzer': 'char', 'count__ngram_range': (1, 3)}\nBest score:\n0.5811496002999502\n/usr/local/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  \"the number of iterations.\", ConvergenceWarning)\n"
        }
      ],
      "source": [
        "tokenize_test_pipeline(model='lr')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "44mb67f82CGD"
      },
      "source": [
        "### Translated ENG SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed: 33.3min\n[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed: 88.0min finished\n[LibLinear]Linear SVM\n------------------\nScores: \n{'mean_fit_time': array([ 199.86388612,  204.61199472,  208.59291446, 1194.05278988,\n       1540.43381255,  744.51242266]), 'std_fit_time': array([ 4.43497799,  4.7003657 ,  2.30412426, 30.52803481, 86.92077743,\n       99.0820755 ]), 'mean_score_time': array([2.19013524, 3.8466135 , 5.83341744, 0.46746995, 1.39227376,\n       2.02988892]), 'std_score_time': array([0.29277496, 0.38413285, 0.81796802, 0.01848559, 0.19368091,\n       0.78178129]), 'param_count__analyzer': masked_array(data=['word', 'word', 'word', 'char', 'char', 'char'],\n             mask=[False, False, False, False, False, False],\n       fill_value='?',\n            dtype=object), 'param_count__ngram_range': masked_array(data=[(1, 1), (1, 2), (1, 3), (1, 1), (1, 2), (1, 3)],\n             mask=[False, False, False, False, False, False],\n       fill_value='?',\n            dtype=object), 'params': [{'count__analyzer': 'word', 'count__ngram_range': (1, 1)}, {'count__analyzer': 'word', 'count__ngram_range': (1, 2)}, {'count__analyzer': 'word', 'count__ngram_range': (1, 3)}, {'count__analyzer': 'char', 'count__ngram_range': (1, 1)}, {'count__analyzer': 'char', 'count__ngram_range': (1, 2)}, {'count__analyzer': 'char', 'count__ngram_range': (1, 3)}], 'split0_test_score': array([0.46119017, 0.46377749, 0.46684994, 0.19615136, 0.42124838,\n       0.48803364]), 'split1_test_score': array([0.53016335, 0.53016335, 0.52531134, 0.22464823, 0.48115801,\n       0.55652596]), 'split2_test_score': array([0.52919295, 0.52822255, 0.52401747, 0.25521591, 0.48503962,\n       0.55118874]), 'split3_test_score': array([0.54213165, 0.53194242, 0.53194242, 0.24680576, 0.44476791,\n       0.54795407]), 'split4_test_score': array([0.54504286, 0.53889698, 0.5342067 , 0.22836811, 0.4873039 ,\n       0.55183568]), 'split5_test_score': array([0.54455766, 0.5481158 , 0.54973314, 0.18049491, 0.49765486,\n       0.56817079]), 'split6_test_score': array([0.56655345, 0.55620249, 0.55555556, 0.2249717 , 0.51738638,\n       0.58321203]), 'split7_test_score': array([0.55571729, 0.55895197, 0.55167394, 0.14685428, 0.50396248,\n       0.57092027]), 'split8_test_score': array([0.53194242, 0.53113375, 0.52822255, 0.16723273, 0.47032185,\n       0.54051431]), 'split9_test_score': array([0.49264111, 0.48762737, 0.47857027, 0.20232897, 0.43150574,\n       0.48940644]), 'mean_test_score': array([0.52991329, 0.52750342, 0.52460833, 0.20730719, 0.47403491,\n       0.54477619]), 'std_test_score': array([0.02952589, 0.0283882 , 0.02817249, 0.03322701, 0.03024287,\n       0.03041535]), 'rank_test_score': array([2, 3, 4, 6, 5, 1], dtype=int32)}\nBest params:\n{'count__analyzer': 'char', 'count__ngram_range': (1, 3)}\nBest score:\n0.5447761922426785\n/usr/local/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  \"the number of iterations.\", ConvergenceWarning)\n"
        }
      ],
      "source": [
        "tokenize_test_pipeline(model='svm')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EHQOSXNLKQEF"
      },
      "source": [
        "### Translated CHN Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed: 10.4min\n[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed: 22.5min finished\n[LibLinear]Logistic Regression\n------------------\nScores: \n{'mean_fit_time': array([ 79.85924022, 106.08262389, 142.9701786 , 201.78887875,\n       256.23497031, 229.90636399]), 'std_fit_time': array([ 1.91491963,  4.17896374, 13.28977827, 18.38440116,  7.02918066,\n       59.42484926]), 'mean_score_time': array([2.37760949, 3.90294809, 6.08079286, 0.34631639, 0.70903943,\n       0.72321827]), 'std_score_time': array([0.09669501, 0.14720386, 0.27737206, 0.03173112, 0.05290408,\n       0.24711447]), 'param_count__analyzer': masked_array(data=['word', 'word', 'word', 'char', 'char', 'char'],\n             mask=[False, False, False, False, False, False],\n       fill_value='?',\n            dtype=object), 'param_count__ngram_range': masked_array(data=[(1, 1), (1, 2), (1, 3), (1, 1), (1, 2), (1, 3)],\n             mask=[False, False, False, False, False, False],\n       fill_value='?',\n            dtype=object), 'params': [{'count__analyzer': 'word', 'count__ngram_range': (1, 1)}, {'count__analyzer': 'word', 'count__ngram_range': (1, 2)}, {'count__analyzer': 'word', 'count__ngram_range': (1, 3)}, {'count__analyzer': 'char', 'count__ngram_range': (1, 1)}, {'count__analyzer': 'char', 'count__ngram_range': (1, 2)}, {'count__analyzer': 'char', 'count__ngram_range': (1, 3)}], 'split0_test_score': array([0.5423674 , 0.54333765, 0.54333765, 0.51552393, 0.55336352,\n       0.54818887]), 'split1_test_score': array([0.60520783, 0.61135371, 0.61167718, 0.57965389, 0.61491186,\n       0.61911693]), 'split2_test_score': array([0.61313278, 0.62267508, 0.6204108 , 0.59178392, 0.6323791 ,\n       0.62833576]), 'split3_test_score': array([0.60375222, 0.61200065, 0.61086851, 0.58612324, 0.61976387,\n       0.61523532]), 'split4_test_score': array([0.61507359, 0.61669093, 0.61280932, 0.59420993, 0.62413068,\n       0.61701439]), 'split5_test_score': array([0.60278182, 0.6065017 , 0.60456089, 0.59049005, 0.62962963,\n       0.62914443]), 'split6_test_score': array([0.62865923, 0.62688016, 0.62267508, 0.61038331, 0.64370047,\n       0.63803979]), 'split7_test_score': array([0.62785056, 0.63334951, 0.63076177, 0.60488436, 0.64580301,\n       0.64483261]), 'split8_test_score': array([0.58191816, 0.58709364, 0.58369723, 0.55571729, 0.60359049,\n       0.59792981]), 'split9_test_score': array([0.54132298, 0.54423419, 0.54439592, 0.52353227, 0.56525958,\n       0.56137797]), 'mean_test_score': array([0.59620666, 0.60041172, 0.59851943, 0.57523022, 0.61325322,\n       0.60992159]), 'std_test_score': array([0.02998866, 0.03070505, 0.02977029, 0.03120693, 0.02960997,\n       0.03031884]), 'rank_test_score': array([5, 3, 4, 6, 1, 2], dtype=int32)}\nBest params:\n{'count__analyzer': 'char', 'count__ngram_range': (1, 2)}\nBest score:\n0.613253220709708\n/usr/local/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  \"the number of iterations.\", ConvergenceWarning)\n"
        }
      ],
      "source": [
        "tokenize_test_pipeline(model='lr')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cX4TjcQQKdPS"
      },
      "source": [
        "### Translated CHN SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\nFitting 10 folds for each of 6 candidates, totalling 60 fits\n[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed: 13.3min\n[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed: 19.6min finished\n[LibLinear]Linear SVM\n------------------\nScores: \n{'mean_fit_time': array([143.75854456, 154.623878  , 183.76461287, 157.76418879,\n       131.1755291 , 123.27293158]), 'std_fit_time': array([ 4.44951464,  2.62774771,  3.81153094,  3.69266955,  1.68336143,\n       22.46824012]), 'mean_score_time': array([2.87137532, 4.49891565, 6.832815  , 0.43380499, 0.8085367 ,\n       0.80856166]), 'std_score_time': array([0.19062099, 0.53707166, 0.88287584, 0.02238096, 0.05979435,\n       0.31407861]), 'param_count__analyzer': masked_array(data=['word', 'word', 'word', 'char', 'char', 'char'],\n             mask=[False, False, False, False, False, False],\n       fill_value='?',\n            dtype=object), 'param_count__ngram_range': masked_array(data=[(1, 1), (1, 2), (1, 3), (1, 1), (1, 2), (1, 3)],\n             mask=[False, False, False, False, False, False],\n       fill_value='?',\n            dtype=object), 'params': [{'count__analyzer': 'word', 'count__ngram_range': (1, 1)}, {'count__analyzer': 'word', 'count__ngram_range': (1, 2)}, {'count__analyzer': 'word', 'count__ngram_range': (1, 3)}, {'count__analyzer': 'char', 'count__ngram_range': (1, 1)}, {'count__analyzer': 'char', 'count__ngram_range': (1, 2)}, {'count__analyzer': 'char', 'count__ngram_range': (1, 3)}], 'split0_test_score': array([0.51115783, 0.50743855, 0.50663001, 0.47994825, 0.52457956,\n       0.52425614]), 'split1_test_score': array([0.56930293, 0.57561055, 0.57561055, 0.54552806, 0.57852175,\n       0.58013909]), 'split2_test_score': array([0.57269934, 0.58434417, 0.58078603, 0.55636422, 0.59518033,\n       0.59210739]), 'split3_test_score': array([0.56412745, 0.56234837, 0.56348051, 0.55296781, 0.57496361,\n       0.57949216]), 'split4_test_score': array([0.57172893, 0.58046256, 0.58013909, 0.56331878, 0.5849911 ,\n       0.58482937]), 'split5_test_score': array([0.56590652, 0.56477438, 0.56186317, 0.55490862, 0.58434417,\n       0.58887272]), 'split6_test_score': array([0.58709364, 0.58660844, 0.58935792, 0.57544881, 0.59712114,\n       0.59485687]), 'split7_test_score': array([0.58644671, 0.58757885, 0.58434417, 0.57318454, 0.60148795,\n       0.59776807]), 'split8_test_score': array([0.54180818, 0.54730713, 0.54245512, 0.51819505, 0.5595989 ,\n       0.54795407]), 'split9_test_score': array([0.501213  , 0.50008087, 0.50040433, 0.49199418, 0.51609251,\n       0.51593078]), 'mean_test_score': array([0.55714845, 0.55965539, 0.55850709, 0.54118583, 0.5716881 ,\n       0.57062067]), 'std_test_score': array([0.02821684, 0.03043827, 0.03039239, 0.03156349, 0.02817368,\n       0.02853911]), 'rank_test_score': array([5, 3, 4, 6, 1, 2], dtype=int32)}\nBest params:\n{'count__analyzer': 'char', 'count__ngram_range': (1, 2)}\nBest score:\n0.5716881031932693\n/usr/local/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  \"the number of iterations.\", ConvergenceWarning)\n"
        }
      ],
      "source": [
        "tokenize_test_pipeline(model='svm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ]
}